{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18cfeddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "from numpy.typing import ArrayLike\n",
    "from typing import Type, Union, Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c69d4d2",
   "metadata": {},
   "source": [
    "# Tensor and Parameter classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "f2dcdb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(np.ndarray):\n",
    "\n",
    "    def __new__(cls, data: ArrayLike):\n",
    "        obj = np.asarray(data).view(cls)\n",
    "        return obj\n",
    "    \n",
    "    def __array_ufunc__(self, ufunc, method, *inputs, out=None, **kwargs):\n",
    "        # input validation \n",
    "        Tensor._validate_inputs(inputs)\n",
    "        Tensor._validate_inputs(kwargs.values())\n",
    "        if out is not None: \n",
    "            Tensor._validate_inputs(out)\n",
    "            \n",
    "            # The subclasses of Tensor must be downcasted to np.arrays \n",
    "            out = Tensor._downcast_where_possible(out)\n",
    "        args = Tensor._downcast_where_possible(inputs)\n",
    "        kwargs = Tensor._downcast_where_possible(kwargs)\n",
    "        results = super().__array_ufunc__(ufunc, method, *args, out=out, **kwargs)\n",
    "        \n",
    "        # return output Tensors if possible\n",
    "        if out is not None: \n",
    "            if ufunc.nout == 1: \n",
    "                results = (results, )\n",
    "            results = tuple(result if output is None else output \n",
    "                            for result, output in zip(results, out))\n",
    "            if len(results) == 1: \n",
    "                results = results[0]\n",
    "        \n",
    "        # Upcast to Tensor \n",
    "        return Tensor._upcast_where_possible(results)\n",
    "    \n",
    "    def __array_function__(self, func, types, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Somehow array_function works without the downcasting, yet also without visiting __array_ufunc__\n",
    "        \"\"\"\n",
    "        Tensor._validate_input_types(types)\n",
    "        results = super().__array_function__(func, types, *args, **kwargs)\n",
    "        return Tensor._upcast_where_possible(results)\n",
    "        \n",
    "    @staticmethod \n",
    "    def _downcast_where_possible(iterable):\n",
    "        if isinstance(iterable, dict):\n",
    "            return dict(zip(iterable.keys(), Tensor._downcast_where_possible(iterable.values())))\n",
    "        return tuple(item.view(np.ndarray) if issubclass(type(item), np.ndarray) else item\n",
    "                     for item in iterable)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _upcast_where_possible(results):\n",
    "        \"\"\"\n",
    "        If the result is an np.ndarray subclass, return a Tensor. If it is an iterable, return it with all \n",
    "        subclasses of np.ndarray casted to Tensor. \n",
    "        \"\"\"\n",
    "        # np.array subclass\n",
    "        if issubclass(type(results), np.ndarray):\n",
    "            return results.view(Tensor)\n",
    "        # Iterable\n",
    "        try: \n",
    "            return type(results)(\n",
    "                result.view(Tensor) if issubclass(type(result), np.ndarray) else result \n",
    "                for result in results\n",
    "            )   \n",
    "        # single, non-iterable object - turn to Tensor\n",
    "        except: \n",
    "            return Tensor(results)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _validate_inputs(inputs: Sequence):\n",
    "        types = map(type, inputs) \n",
    "        Tensor._validate_input_types(types)\n",
    "        \n",
    "    @staticmethod \n",
    "    def _validate_input_types(types):\n",
    "        \"\"\"\n",
    "        Does not accept np.ndarray superclasses \n",
    "        \"\"\"\n",
    "        if any(issubclass(Tensor, type_) and not Tensor is type_ for type_ in types):\n",
    "            raise NotImplemented\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "5d8a2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(Tensor):\n",
    "    \"\"\"\n",
    "    Does not override __array_function__ or __array_ufunc__, since we do not want operations on Parameters \n",
    "    to produce new Parameters.\n",
    "    \n",
    "    TODO: Consider whether requires_grad should not always be True, we have Tensors \n",
    "    for objects that do not have gradients.\n",
    "    \"\"\"\n",
    "    def __new__(cls, data: Tensor, requires_grad: bool = True):\n",
    "        obj = data.view(cls)\n",
    "        \n",
    "        # At this point __array_finalize__ has once been executed via the view method; thus, \n",
    "        # if the object is instantiated via constructor, it already has _grad = None\n",
    "        if requires_grad is True: \n",
    "            obj._grad = Tensor(np.zeros(obj.shape))\n",
    "        return obj\n",
    "    \n",
    "    def __array_finalize__(self, obj):\n",
    "        if obj is None: return \n",
    "        self._grad = getattr(obj, '_grad', None)\n",
    "    \n",
    "    @property\n",
    "    def grad(self) -> Tensor:\n",
    "        return self._grad\n",
    "    \n",
    "    @grad.setter\n",
    "    def grad(self, dx: Tensor):\n",
    "        assert dx.shape == self.shape \n",
    "        self._grad = dx\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.grad = Tensor(np.zeros(self.shape))\n",
    "    \n",
    "    def optimize(self, dx: Tensor, *, maximize: bool = False):\n",
    "        \"\"\"\n",
    "        Final optimization step deffered to the Parameter itself. Reason being that we might \n",
    "        want to implement Parameter subclasses with their own constraints on the descent/ascent steps. \n",
    "        \"\"\"\n",
    "        if maximize is True: \n",
    "            self += dx \n",
    "        else: \n",
    "            self -= dx \n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{super().__repr__()} with gradient: {repr(self.grad)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "624cfff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor([1, 2, 3, 4, 5, 6]), Tensor([1, 2, 3, 4, 5, 6]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = Tensor([0, 0, 0, 0, 0, 0])\n",
    "\n",
    "np.concatenate((Tensor([1, 2, 3]), Tensor([4, 5, 6])), out=foo), foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "da012c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([3., 4.])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = Tensor([1, 2]).astype(float)\n",
    "foo = foo + 1\n",
    "foo += 1\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "496ddc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parameter():\n",
    "    bar = Parameter(Tensor([1, 2])).astype(float)\n",
    "    bar.grad += 1\n",
    "    car = bar.copy() \n",
    "    car -= bar.grad \n",
    "    bar.optimize(Tensor([1., 1.]))\n",
    "    assert tuple(bar) == (0.0, 1.0)\n",
    "    assert tuple(bar.grad) == (1.0, 1.0)\n",
    "    assert isinstance(car, Tensor)\n",
    "    bar.zero_grad()\n",
    "    assert tuple(bar.grad) == (0.0, 0.0)\n",
    "    \n",
    "test_parameter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d228e6ed",
   "metadata": {},
   "source": [
    "# Module with parameter registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "472e2f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Tuple, Iterator\n",
    "\n",
    "class Module(ABC):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._parameters = None\n",
    "        self._modules = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *args, **kwargs) -> Any:\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, *args, **kwargs) -> Any:\n",
    "        ...\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> Any:\n",
    "        return self.forward(*args, **kwargs)\n",
    "    \n",
    "    def named_modules(self, recurse: bool = True) -> Iterator[Tuple[str, Module]]:\n",
    "        \"\"\"\n",
    "        Generates submodules and their names of this Module object in the order in which they appear \n",
    "        in the __dict__ attribute. If recurse is True then subsubmodules of submodules etc. until reaching \n",
    "        submodules with no subsubmodules. \n",
    "        \"\"\"\n",
    "        if self._modules is None:\n",
    "            self._register_modules_from_attributes()\n",
    "        for name, module in self._modules.items():\n",
    "            yield name, module\n",
    "            if recurse is True:\n",
    "                for subname, submodule in module.named_modules():\n",
    "                    yield f\"{name}.{subname}\", submodule\n",
    "                    \n",
    "    def modules(self, recurse: bool = True) -> Iterator[Module]:\n",
    "        for _, module in self.named_modules(recurse=recurse): \n",
    "            yield module\n",
    "\n",
    "    def named_parameters(self, recurse: bool = True) -> Iterator[Tuple[str, Parameter]]:\n",
    "        if self._parameters is None:\n",
    "            self._register_parameters_from_attributes()\n",
    "        for name, param in self._parameters.items():\n",
    "            yield name, param\n",
    "        if recurse is True:\n",
    "            for module_name, module in self.named_modules(recurse=False):\n",
    "                for param_name, param in module.named_parameters(recurse=True):\n",
    "                    yield f\"{module_name}.{param_name}\", param\n",
    "                    \n",
    "    def parameters(self, recurse: bool = True) -> Iterator[Parameter]:\n",
    "        for _, parameter in self.named_parameters(recurse=recurse):\n",
    "            yield parameter\n",
    "\n",
    "    def _register_modules_from_attributes(self) -> None:\n",
    "        self._modules = {attr: val for attr, val in self.__dict__.items()\n",
    "                         if issubclass(type(val), Module)}\n",
    "\n",
    "    def _register_parameters_from_attributes(self) -> None:\n",
    "        self._parameters = {attr: val for attr, val in self.__dict__.items() \n",
    "                            if issubclass(type(val), Parameter)}\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "1f507e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Module):\n",
    "    \"\"\"\n",
    "    Addition operation between two Tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x1: Tensor, x2: Tensor) -> Tensor:\n",
    "        return x1 + x2\n",
    "\n",
    "    def backward(self, dy: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        return dy, dy\n",
    "    \n",
    "    \n",
    "class MatMul(Module):\n",
    "    \"\"\"\n",
    "    Matrix multiplication of two Tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.x1: Tensor = None\n",
    "        self.x2: Tensor = None\n",
    "\n",
    "    def forward(self, x1: Tensor, x2: Tensor) -> Tensor:\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "        return self.x1 @ self.x2\n",
    "\n",
    "    def backward(self, dy: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        dx1 = dy @ self.x2.T\n",
    "        dx2 = self.x1.T @ dy\n",
    "        return dx1, dx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "4f2f9843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "def normal(shape, mean: float = 0.0, std: float = 1.0):\n",
    "    return rng.normal(loc=mean, scale=std, size=shape)\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.weights = Parameter(normal((self.out_features, self.in_features)))\n",
    "        self.matmul = MatMul()\n",
    "        \n",
    "        if bias is True:\n",
    "            self.bias = Parameter(normal((self.out_features, 1))) if bias else None\n",
    "            self.add = Add()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        y = self.matmul(self.weights, x)\n",
    "        if self.bias is not None: \n",
    "            y = self.add(self.bias, y)\n",
    "        return y\n",
    "\n",
    "    def backward(self, dy):\n",
    "        if self.bias is not None:\n",
    "            db, dy = self.add.backward(dy)\n",
    "            self.bias.grad += db\n",
    "            \n",
    "        dw, dx = self.matmul.backward(dy)\n",
    "        self.weights.grad += dw\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "c154868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "    \n",
    "    def __init__(self, in_features: int, hidden_features: int, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.lin1 = Linear(in_features, hidden_features, bias=bias)\n",
    "        self.lin2 = Linear(hidden_features, out_features, bias=bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        y = self.lin2(x)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        dy = self.lin2.backward(dy=dy)\n",
    "        dx = self.lin1.backward(dy=dy)\n",
    "        return dx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "b1b32034",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMLP(Module):\n",
    "    \n",
    "    def __init__(self, features1, features2, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.mlp1 = MLP(*features1, bias)\n",
    "        self.mlp2 = MLP(*features2, bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.mlp1(x)\n",
    "        y = self.mlp2(x)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        dy = self.mlp2.backward(dy)\n",
    "        dx = self.mlp1.backward(dy)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "d33eb4e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='weights', parameter=Parameter([[-0.49431333],\n",
      "           [ 0.17091917]])\n",
      "name='bias', parameter=Parameter([[ 0.69337367],\n",
      "           [-0.38871296]])\n",
      "name='matmul', module=<__main__.MatMul object at 0x00000183F8AA1C10>\n",
      "name='add', module=<__main__.Add object at 0x00000183F8AA1580>\n"
     ]
    }
   ],
   "source": [
    "model = Linear(1, 2)\n",
    "\n",
    "for name, parameter in model.named_parameters(): \n",
    "    print(f\"{name=}, {parameter=}\")\n",
    "    \n",
    "for name, module in model.named_modules():\n",
    "    print(f\"{name=}, {module=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "2f309dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor([[ 0.19906035],\n",
       "         [-0.21779379]]),\n",
       " Tensor([[-0.32339416]]))"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(Tensor([[1.0]])), model.backward(Tensor([[1.0], [1.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2389891",
   "metadata": {},
   "source": [
    "# Optimizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "327dad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "class Optimizer(ABC):\n",
    "    \n",
    "    def __init__(self, params: Union[Iterator[Tuple[str, Parameter]], Dict[str, Any]], \n",
    "                 **defaults):\n",
    "        self.param_groups = Optimizer._group_params(params=params, defaults=defaults)\n",
    "         \n",
    "    def _group_params(params, defaults: Union[dict, None] = None) -> list[dict]:\n",
    "         # Convert iterable of parameters into list of dictionaries\n",
    "        param_groups = list(params) \n",
    "        if all(isinstance(item, Parameter) for item in param_groups):\n",
    "            param_groups = [{'params': param_groups}]\n",
    "            \n",
    "        # Convert single Parameters into a list of one Parameter and other iterables into lists \n",
    "        for param_group in param_groups:   \n",
    "            params = param_group['params']\n",
    "            if isinstance(params, Parameter):\n",
    "                params = [params]\n",
    "            else:\n",
    "                params = list(params)\n",
    "                if len(params) == 0:\n",
    "                    raise ValueError(\"Empty parameter list passed to Optimizer\")\n",
    "            param_group['params'] = params\n",
    "            \n",
    "        # Add defaults if present \n",
    "        if defaults is not None: \n",
    "            for param_group in param_groups:\n",
    "                keys_present = param_group.keys() \n",
    "                param_group.update({key: val for key, val in defaults.items()\n",
    "                                    if key not in keys_present})\n",
    "        return param_groups\n",
    "        \n",
    "    @abstractmethod \n",
    "    def step(self):\n",
    "        ... \n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        for param_group in self.param_groups:\n",
    "            for param in param_group['params']:\n",
    "                param.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "b830c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "class LRScheduler(ABC):\n",
    "    \n",
    "    def __init__(self, epoch: int = -1) -> None:\n",
    "        self.epoch = epoch\n",
    "    \n",
    "    def step(self, lr: float) -> float:\n",
    "        self.epoch += 1\n",
    "        if self.epoch == 0:\n",
    "            return self._setup(lr=lr)\n",
    "        else:\n",
    "            return self._step(lr=lr)\n",
    "        \n",
    "    @abstractmethod \n",
    "    def _step(self, lr: float) -> float: \n",
    "        ...\n",
    "        \n",
    "    def _setup(self, lr: float) -> float: \n",
    "        return lr \n",
    "    \n",
    "    \n",
    "class LambdaLR(LRScheduler):\n",
    "    \"\"\"\n",
    "    Always returns initial learning rate times lr_lambda(epoch). \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr_lambda: Callable[[int], float], epoch: int = -1) -> None:\n",
    "        super().__init__(epoch=epoch)\n",
    "        self.lr_lambda = lr_lambda\n",
    "        self.base_lr: Union[float, None] = None\n",
    "\n",
    "    def _step(self, lr: float) -> float: \n",
    "        return self.base_lr * self.lr_lambda(self.epoch)\n",
    "    \n",
    "    def _setup(self, lr: float) -> float: \n",
    "        self.base_lr = lr\n",
    "        return lr\n",
    "    \n",
    "\n",
    "class MultiplicativeLR(LRScheduler):\n",
    "    \n",
    "    def __init__(self, lr_lambda: Callable[[int], float], epoch: int = -1) -> None:\n",
    "        super().__init__(epoch=epoch)\n",
    "        self.lr_lambda = lr_lambda\n",
    "    \n",
    "    def _step(self, lr: float) -> float: \n",
    "        return lr * self.lr_lambda(self.epoch)\n",
    "        \n",
    "    \n",
    "class StepLR(LRScheduler):\n",
    "    \n",
    "    # TODO perhaps rename step_size to step_epochs \n",
    "    def __init__(self, step_size: int, gamma: float = 0.1, epoch: int = -1) -> None: \n",
    "        super().__init__(epoch=epoch)\n",
    "        self.step_size = step_size \n",
    "        self.gamma = gamma \n",
    "    \n",
    "    def _step(self, lr: float) -> float: \n",
    "        if self.epoch % self.step_size == 0: \n",
    "            return lr * self.gamma \n",
    "        return lr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "5f5524c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce \n",
    "\n",
    "\n",
    "class LROptimizer(Optimizer):\n",
    "    \n",
    "    def __init__(self, params: Union[Iterator[Tuple[str, Parameter]], Dict[str, Any]], lr: float, \n",
    "                 lr_schedulers: Union[list[LRScheduler], LRScheduler, None] = None, \n",
    "                 **defaults: dict[str, Any]) -> None:\n",
    "        super().__init__(params=params, lr=lr, lr_schedulers=lr_schedulers, **defaults)\n",
    "        self._prep_lr_schedulers()\n",
    "        self.schedule_lr()\n",
    "         \n",
    "    def _prep_lr_schedulers(self) -> None:\n",
    "        \"\"\"\n",
    "        Ensures that the 'lr_schedulers' attributes of each param_group is a list. \n",
    "        \"\"\"\n",
    "        for param_group in self.param_groups:\n",
    "            lr_schedulers = param_group['lr_schedulers']\n",
    "            # TODO another solution would be to have an identity scheduler class and set it here \n",
    "            if lr_schedulers is None: \n",
    "                continue \n",
    "            elif isinstance(lr_schedulers, LRScheduler):\n",
    "                lr_schedulers = [lr_schedulers]\n",
    "            else:\n",
    "                lr_schedulers = list(lr_schedulers)\n",
    "            param_group['lr_schedulers'] = lr_schedulers\n",
    "        \n",
    "    @abstractmethod\n",
    "    def step(self) -> None:\n",
    "        ... \n",
    "        \n",
    "    def schedule_lr(self) -> None:\n",
    "        \"\"\"\n",
    "        Applies all lr_schedulers in a given param_group to the learning rate of that group.\n",
    "        \"\"\"\n",
    "        for param_group in self.param_groups:\n",
    "            lr_schedulers = param_group['lr_schedulers']\n",
    "            # It is possible that one param group has a scheduler, while another does not\n",
    "            if lr_schedulers is not None:\n",
    "                param_group['lr'] = reduce(\n",
    "                    lambda lr, lr_scheduler: lr_scheduler.step(lr), \n",
    "                    param_group['lr_schedulers'], \n",
    "                    param_group['lr'],\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "id": "bdec7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(LROptimizer):\n",
    "    def __init__(self, params, lr: float, \n",
    "                 lr_schedulers: Union[list[LRScheduler], LRScheduler, None] = None, \n",
    "                 momentum: float = 0, dampening: float = 0, weight_decay: float = 0, \n",
    "                 nesterov: bool = False, *, maximize: bool = False):\n",
    "        super().__init__(params=params, lr=lr, lr_schedulers=lr_schedulers, momentum=momentum, \n",
    "                         dampening=dampening, weight_decay=weight_decay, nesterov=nesterov, \n",
    "                         maximize=maximize)\n",
    "        # store exponential moving average of the gradient if using momentum\n",
    "        for param_group in self.param_groups:\n",
    "            param_group['grads_ema'] = [param.grad if momentum != 0 else None \n",
    "                                        for param in param_group['params']]\n",
    "            \n",
    "    def step(self):\n",
    "        for param_group in self.param_groups:\n",
    "            lr, momentum, dampening, weight_decay, nesterov, maximize = param_group['lr'], \\\n",
    "            param_group['momentum'], param_group['dampening'], param_group['weight_decay'], \\\n",
    "            param_group['nesterov'], param_group['maximize']\n",
    "            for param, grad_ema in zip(param_group['params'], param_group['grads_ema']):\n",
    "                SGD.sgd(param=param, grad_ema=grad_ema, lr=lr, momentum=momentum, dampening=dampening, \n",
    "                       weight_decay=weight_decay, nesterov=nesterov, maximize=maximize)\n",
    "                \n",
    "                \n",
    "    @staticmethod\n",
    "    def sgd(param: Parameter, grad_ema: list[Tensor], lr: float, momentum: float = 0, dampening: float = 0, \n",
    "            weight_decay: float = 0, nesterov: bool = False, *, maximize: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Applies Stochastic Gradient Descent (or Ascent) on a given Parameter.\n",
    "        \n",
    "        TODO: consider whether copy should be optional to save space. \n",
    "        \"\"\"\n",
    "        grad = param.grad.copy()\n",
    "        # L2 penalty \n",
    "        if weight_decay != 0:\n",
    "            grad += weight_decay * param \n",
    "        # momentum \n",
    "        if momentum != 0:\n",
    "            grad_ema = momentum * grad_ema + (1 - dampening) * grad \n",
    "        # Adjustment for Nesterov's version of momentum SGD\n",
    "        if nesterov is True: \n",
    "            grad += momentum * grad_ema \n",
    "        # Apply learning rate \n",
    "        param.optimize(grad * lr, maximize=maximize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "id": "af7b168d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter([[-0.42285552],\n",
       "              [-0.56282902]]) with gradient: Tensor([[1.],\n",
       "           [1.]]),\n",
       "   Parameter([[-0.65307242],\n",
       "              [ 0.53625914]]) with gradient: Tensor([[1.],\n",
       "           [1.]])],\n",
       "  'lr': 0.01,\n",
       "  'lr_schedulers': [<__main__.StepLR at 0x183fce5beb0>],\n",
       "  'momentum': 0.9,\n",
       "  'dampening': 0,\n",
       "  'weight_decay': 0.2,\n",
       "  'nesterov': True,\n",
       "  'maximize': True,\n",
       "  'grads_ema': [Tensor([[1.],\n",
       "           [1.]]),\n",
       "   Tensor([[1.],\n",
       "           [1.]])]}]"
      ]
     },
     "execution_count": 819,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Linear(1, 2)\n",
    "optimizer = SGD(model.parameters(), 0.01, lr_schedulers=StepLR(2, 0.1), momentum=0.9, dampening=0, \n",
    "                weight_decay=0.2, nesterov=True, \n",
    "                maximize=True)\n",
    "model(Tensor([[1.0]]))\n",
    "model.backward(Tensor([[1.0], [1.0]]))\n",
    "optimizer.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "id": "11890e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter([[-0.42285552],\n",
       "              [-0.56282902]]) with gradient: Tensor([[1.],\n",
       "           [1.]]),\n",
       "   Parameter([[-0.65307242],\n",
       "              [ 0.53625914]]) with gradient: Tensor([[1.],\n",
       "           [1.]])],\n",
       "  'lr': 0.01,\n",
       "  'lr_schedulers': [<__main__.StepLR at 0x183fce5beb0>],\n",
       "  'momentum': 0.9,\n",
       "  'dampening': 0,\n",
       "  'weight_decay': 0.2,\n",
       "  'nesterov': True,\n",
       "  'maximize': True,\n",
       "  'grads_ema': [Tensor([[1.],\n",
       "           [1.]]),\n",
       "   Tensor([[1.],\n",
       "           [1.]])]}]"
      ]
     },
     "execution_count": 820,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.schedule_lr()\n",
    "optimizer.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "id": "cb9e4029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter([[-0.42285552],\n",
      "           [-0.56282902]]) with gradient: Tensor([[1.],\n",
      "        [1.]])\n",
      "Parameter([[-0.65307242],\n",
      "           [ 0.53625914]]) with gradient: Tensor([[1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(repr(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "a563d2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter([[-0.39736238],\n",
      "           [-0.53786777]]) with gradient: Tensor([[1.],\n",
      "        [1.]])\n",
      "Parameter([[-0.6284541 ],\n",
      "           [ 0.56539692]]) with gradient: Tensor([[1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "optimizer.step()\n",
    "for param in model.parameters():\n",
    "    print(repr(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "id": "c3f4bdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.39736238]\n",
      " [-0.53786777]] [[0.]\n",
      " [0.]]\n",
      "[[-0.6284541 ]\n",
      " [ 0.56539692]] [[0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "for param in model.parameters():\n",
    "    print(param, param.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c944cc4a",
   "metadata": {},
   "source": [
    "# Example of everything together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "c3f9ee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.x_hat = None\n",
    "\n",
    "    def forward(self, x, x_hat):\n",
    "        self.x = x\n",
    "        self.x_hat = x_hat\n",
    "        return np.mean((x - x_hat) ** 2, keepdims=True)\n",
    "\n",
    "    def backward(self, dy=1):\n",
    "        return 2 * (self.x_hat - self.x) / len(self.x) * dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "id": "9ac54c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [Tensor(normal(shape=(2, 1))) for i in range(100)]\n",
    "targets = [t * 2 for t in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "id": "02c78155",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Linear(2, 2)\n",
    "model2 = Linear(2, 2)\n",
    "model2.weights = model1.weights.copy() \n",
    "model2.bias = model1.bias.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "id": "ae48d769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 23.225985366989548\r",
      "Epoch: 1, loss: 0.3824730671822267\r",
      "Epoch: 1, loss: 7.566950172656281\r",
      "Epoch: 1, loss: 11.562475835419681\r",
      "Epoch: 1, loss: 4.072047958561216\r",
      "Epoch: 1, loss: 30.599484923127026\r",
      "Epoch: 1, loss: 11.024411423888916\r",
      "Epoch: 1, loss: 8.215936759521933\r",
      "Epoch: 1, loss: 1.1001666745987908\r",
      "Epoch: 1, loss: 8.403307765724616\r",
      "Epoch: 1, loss: 2.6105182605044934\r",
      "Epoch: 1, loss: 5.03573852788596\r",
      "Epoch: 1, loss: 12.911750140372494\r",
      "Epoch: 1, loss: 0.9409447423308067\r",
      "Epoch: 1, loss: 7.498842093813444\r",
      "Epoch: 1, loss: 13.542841172255915\r",
      "Epoch: 1, loss: 7.58463486286154\r",
      "Epoch: 1, loss: 5.083520540477767\r",
      "Epoch: 1, loss: 4.196999603830334\r",
      "Epoch: 1, loss: 3.724230716036211\r",
      "Epoch: 1, loss: 0.03465488065893461\r",
      "Epoch: 1, loss: 3.2845331687089394\r",
      "Epoch: 1, loss: 4.753975400005972\r",
      "Epoch: 1, loss: 5.066481298058701\r",
      "Epoch: 1, loss: 1.1761425658382323\r",
      "Epoch: 1, loss: 1.5763466450768233\r",
      "Epoch: 1, loss: 1.936204899478467\r",
      "Epoch: 1, loss: 1.9374080840304615\r",
      "Epoch: 1, loss: 0.12148844068553949\r",
      "Epoch: 1, loss: 0.5665787857422809\r",
      "Epoch: 1, loss: 45.538779047068914\r",
      "Epoch: 1, loss: 6.139660555098363\r",
      "Epoch: 1, loss: 1.0444510911204319\r",
      "Epoch: 1, loss: 0.4839733641836094\r",
      "Epoch: 1, loss: 1.160988369335526\r",
      "Epoch: 1, loss: 6.510061816199334\r",
      "Epoch: 1, loss: 0.692105867739085\r",
      "Epoch: 1, loss: 0.6271206514952474\r",
      "Epoch: 1, loss: 0.6205293458443096\r",
      "Epoch: 1, loss: 7.2297074183768455\r",
      "Epoch: 1, loss: 0.0009791582334031654\r",
      "Epoch: 1, loss: 0.39363592856967944\r",
      "Epoch: 1, loss: 13.827631229492248\r",
      "Epoch: 1, loss: 9.535526710243609\r",
      "Epoch: 1, loss: 1.472563442671171\r",
      "Epoch: 1, loss: 5.667330492047816\r",
      "Epoch: 1, loss: 0.750537011261228\r",
      "Epoch: 1, loss: 3.478753698540003\r",
      "Epoch: 1, loss: 2.6282071162549485\r",
      "Epoch: 1, loss: 0.8545086101526771\r",
      "Epoch: 1, loss: 0.21016096997160338\r",
      "Epoch: 1, loss: 2.467924388779115\r",
      "Epoch: 1, loss: 0.001202298986477396\r",
      "Epoch: 1, loss: 0.5154045814281206\r",
      "Epoch: 1, loss: 0.5184698756565737\r",
      "Epoch: 1, loss: 0.31989784554634615\r",
      "Epoch: 1, loss: 0.3103181477795624\r",
      "Epoch: 1, loss: 1.4349369826300484\r",
      "Epoch: 1, loss: 3.074860933852\r",
      "Epoch: 1, loss: 0.08403285753838893\r",
      "Epoch: 1, loss: 2.9819654315327706\r",
      "Epoch: 1, loss: 3.117576619078956\r",
      "Epoch: 1, loss: 0.05492250207319153\r",
      "Epoch: 1, loss: 0.5216695335625839\r",
      "Epoch: 1, loss: 8.116735714860356\r",
      "Epoch: 1, loss: 5.551344393685215\r",
      "Epoch: 1, loss: 1.3008731647040026\r",
      "Epoch: 1, loss: 0.7064762716441153\r",
      "Epoch: 1, loss: 2.5668091147407766\r",
      "Epoch: 1, loss: 2.5018302860418076\r",
      "Epoch: 1, loss: 3.432436537515847\r",
      "Epoch: 1, loss: 5.996839613265994\r",
      "Epoch: 1, loss: 3.133664784068495\r",
      "Epoch: 1, loss: 1.4823476268809601\r",
      "Epoch: 1, loss: 1.4548758746593797\r",
      "Epoch: 1, loss: 2.1741770068459614\r",
      "Epoch: 1, loss: 0.1342915954805054\r",
      "Epoch: 1, loss: 3.0745453974065926\r",
      "Epoch: 1, loss: 0.6738717908810061\r",
      "Epoch: 1, loss: 0.16194573566607448\r",
      "Epoch: 1, loss: 2.550362520321195\r",
      "Epoch: 1, loss: 5.888327438576869\r",
      "Epoch: 1, loss: 0.5267118404533959\r",
      "Epoch: 1, loss: 1.4870380305387163\r",
      "Epoch: 1, loss: 3.221873330279164\r",
      "Epoch: 1, loss: 5.379782760212867\r",
      "Epoch: 1, loss: 1.460417952985021\r",
      "Epoch: 1, loss: 0.635033876425097\r",
      "Epoch: 1, loss: 0.6684224060450792\r",
      "Epoch: 1, loss: 1.6490953309136787\r",
      "Epoch: 1, loss: 1.3527695242195081\r",
      "Epoch: 1, loss: 0.0963279662510116\r",
      "Epoch: 1, loss: 0.6590933705264577\r",
      "Epoch: 1, loss: 0.8035226628884322\r",
      "Epoch: 1, loss: 0.18345546994536663\r",
      "Epoch: 1, loss: 0.2362958005267583\r",
      "Epoch: 1, loss: 0.06413047840316871\r",
      "Epoch: 1, loss: 0.03339472282420093\r",
      "Epoch: 1, loss: 1.6057520727228611\r",
      "Epoch: 1, loss: 0.19681670680221253\r",
      "Epoch: 2, loss: 2.5178188496350336\r",
      "Epoch: 2, loss: 0.0658452222717454\r",
      "Epoch: 2, loss: 1.0572215325642595\r",
      "Epoch: 2, loss: 1.5765317923435165\r",
      "Epoch: 2, loss: 0.6003157879311396\r",
      "Epoch: 2, loss: 1.9812770619206417\r",
      "Epoch: 2, loss: 0.8507377697893932\r",
      "Epoch: 2, loss: 0.9235189605836475\r",
      "Epoch: 2, loss: 0.1232940677887495\r",
      "Epoch: 2, loss: 0.9408529205285562\r",
      "Epoch: 2, loss: 0.2479266413114974\r",
      "Epoch: 2, loss: 0.38022670247903384\r",
      "Epoch: 2, loss: 1.2208637808477778\r",
      "Epoch: 2, loss: 0.1310477322671919\r",
      "Epoch: 2, loss: 0.891211790390738\r",
      "Epoch: 2, loss: 0.9601866922124368\r",
      "Epoch: 2, loss: 1.0485930830632313\r",
      "Epoch: 2, loss: 0.6431626902772652\r",
      "Epoch: 2, loss: 0.6113787729274376\r",
      "Epoch: 2, loss: 0.4370047735158859\r",
      "Epoch: 2, loss: 0.009456308184365958\r",
      "Epoch: 2, loss: 0.4472412713599141\r",
      "Epoch: 2, loss: 0.39077265967856983\r",
      "Epoch: 2, loss: 0.26003325080690737\r",
      "Epoch: 2, loss: 0.1618162984932398\r",
      "Epoch: 2, loss: 0.07162699293783276\r",
      "Epoch: 2, loss: 0.18192002250732492\r",
      "Epoch: 2, loss: 0.16922552973351151\r",
      "Epoch: 2, loss: 0.0043344335406090406\r",
      "Epoch: 2, loss: 0.05007671202123827\r",
      "Epoch: 2, loss: 4.137906840037687\r",
      "Epoch: 2, loss: 0.4930000094630322\r",
      "Epoch: 2, loss: 0.1736266749353413\r",
      "Epoch: 2, loss: 0.04260149849808855\r",
      "Epoch: 2, loss: 0.12641933315633863\r",
      "Epoch: 2, loss: 0.5054816549922874\r",
      "Epoch: 2, loss: 0.09787076851465405\r",
      "Epoch: 2, loss: 0.05420718492163836\r",
      "Epoch: 2, loss: 0.09687165990689\r",
      "Epoch: 2, loss: 0.6057410888111183\r",
      "Epoch: 2, loss: 0.004514723854385405\r",
      "Epoch: 2, loss: 0.020875388896989554\r",
      "Epoch: 2, loss: 1.5381032948959081\r",
      "Epoch: 2, loss: 1.2679608526302535\r",
      "Epoch: 2, loss: 0.10903483139519102\r",
      "Epoch: 2, loss: 0.5191264385162097\r",
      "Epoch: 2, loss: 0.062389316009375625\r",
      "Epoch: 2, loss: 0.48864185934913185\r",
      "Epoch: 2, loss: 0.17757003994849407\r",
      "Epoch: 2, loss: 0.14132760606048528\r",
      "Epoch: 2, loss: 0.04492042531252018\r",
      "Epoch: 2, loss: 0.3529631464654419\r",
      "Epoch: 2, loss: 0.005488553008193556\r",
      "Epoch: 2, loss: 0.04587637065778979\r",
      "Epoch: 2, loss: 0.04447962858123879\r",
      "Epoch: 2, loss: 0.044565402515031466\r",
      "Epoch: 2, loss: 0.033291545620474046\r",
      "Epoch: 2, loss: 0.110801339338501\r",
      "Epoch: 2, loss: 0.3855387019604928\r",
      "Epoch: 2, loss: 0.008267518813709896\r",
      "Epoch: 2, loss: 0.2736396124737157\r",
      "Epoch: 2, loss: 0.417230955329722\r",
      "Epoch: 2, loss: 0.0166817479274852\r",
      "Epoch: 2, loss: 0.04746954163030203\r",
      "Epoch: 2, loss: 0.8142214264520065\r",
      "Epoch: 2, loss: 0.6916019715131254\r",
      "Epoch: 2, loss: 0.18044610953026965\r",
      "Epoch: 2, loss: 0.05804454160276544\r",
      "Epoch: 2, loss: 0.23405351358211704\r",
      "Epoch: 2, loss: 0.16059317024492298\r",
      "Epoch: 2, loss: 0.34756679099505616\r",
      "Epoch: 2, loss: 0.6644397874899952\r",
      "Epoch: 2, loss: 0.34161601266058617\r",
      "Epoch: 2, loss: 0.2130617928559183\r",
      "Epoch: 2, loss: 0.10924121102280196\r",
      "Epoch: 2, loss: 0.3077412358472884\r",
      "Epoch: 2, loss: 0.012757958631570917\r",
      "Epoch: 2, loss: 0.35661237902949666\r",
      "Epoch: 2, loss: 0.09441496404182356\r",
      "Epoch: 2, loss: 0.030620569539042245\r",
      "Epoch: 2, loss: 0.35318699550304067\r",
      "Epoch: 2, loss: 0.6941158482069261\r",
      "Epoch: 2, loss: 0.07042280195486211\r",
      "Epoch: 2, loss: 0.21197018403437445\r",
      "Epoch: 2, loss: 0.380700835274821\r",
      "Epoch: 2, loss: 0.6479514558665275\r",
      "Epoch: 2, loss: 0.1301422276137773\r",
      "Epoch: 2, loss: 0.050731931561834406\r",
      "Epoch: 2, loss: 0.1014896139740935\r",
      "Epoch: 2, loss: 0.17200259131231765\r",
      "Epoch: 2, loss: 0.12849366804614357\r",
      "Epoch: 2, loss: 0.014256693149903629\r",
      "Epoch: 2, loss: 0.09156915648749722\r",
      "Epoch: 2, loss: 0.0815534293759669\r",
      "Epoch: 2, loss: 0.030954713634051584\r",
      "Epoch: 2, loss: 0.03858871221891182\r",
      "Epoch: 2, loss: 0.009948608239875947\r",
      "Epoch: 2, loss: 0.00559009680522191\r",
      "Epoch: 2, loss: 0.22488219720667516\r",
      "Epoch: 2, loss: 0.016286296862205934\r",
      "Epoch: 3, loss: 0.2898476960607952\r",
      "Epoch: 3, loss: 0.011292084946190984\r",
      "Epoch: 3, loss: 0.15136954989137114\r",
      "Epoch: 3, loss: 0.21780785564409433\r",
      "Epoch: 3, loss: 0.08631756337652309\r",
      "Epoch: 3, loss: 0.1377379765001618\r",
      "Epoch: 3, loss: 0.07421517290058917\r",
      "Epoch: 3, loss: 0.11047012946655882\r",
      "Epoch: 3, loss: 0.015719675350328133\r",
      "Epoch: 3, loss: 0.11205561366191678\r",
      "Epoch: 3, loss: 0.023006234014175807\r",
      "Epoch: 3, loss: 0.03078262582295101\r",
      "Epoch: 3, loss: 0.12476395752925874\r",
      "Epoch: 3, loss: 0.024226367892451608\r",
      "Epoch: 3, loss: 0.11033801065560303\r",
      "Epoch: 3, loss: 0.07544776141878129\r",
      "Epoch: 3, loss: 0.143178497720478\r",
      "Epoch: 3, loss: 0.08326632056614267\r",
      "Epoch: 3, loss: 0.08721293753441542\r",
      "Epoch: 3, loss: 0.054413120540224354\r",
      "Epoch: 3, loss: 0.0029591009522018682\r",
      "Epoch: 3, loss: 0.06098064008974787\r",
      "Epoch: 3, loss: 0.03660647346268545\r",
      "Epoch: 3, loss: 0.023935092732243805\r",
      "Epoch: 3, loss: 0.02289181339316392\r",
      "Epoch: 3, loss: 0.008123696710707904\r",
      "Epoch: 3, loss: 0.01692387304089041\r",
      "Epoch: 3, loss: 0.017077737009175985\r",
      "Epoch: 3, loss: 0.0009537359392396579\r",
      "Epoch: 3, loss: 0.004553642383504122\r",
      "Epoch: 3, loss: 0.410917869073501\r",
      "Epoch: 3, loss: 0.044759522161819915\r",
      "Epoch: 3, loss: 0.028981167097526003\r",
      "Epoch: 3, loss: 0.0038446300426807586\r",
      "Epoch: 3, loss: 0.02100637712365081\r",
      "Epoch: 3, loss: 0.04087696444930254\r",
      "Epoch: 3, loss: 0.014219193461107537\r",
      "Epoch: 3, loss: 0.0048084463051782655\r",
      "Epoch: 3, loss: 0.014817117549778823\r",
      "Epoch: 3, loss: 0.0529768702353018\r",
      "Epoch: 3, loss: 0.002069051155876508\r",
      "Epoch: 3, loss: 0.001152994800349868\r",
      "Epoch: 3, loss: 0.17937330358945355\r",
      "Epoch: 3, loss: 0.16788494813387475\r",
      "Epoch: 3, loss: 0.008898907464524276\r",
      "Epoch: 3, loss: 0.05318741212691063\r",
      "Epoch: 3, loss: 0.005907924279779682\r",
      "Epoch: 3, loss: 0.06771179273416014\r",
      "Epoch: 3, loss: 0.012781090467601518\r",
      "Epoch: 3, loss: 0.02228894622317064\r",
      "Epoch: 3, loss: 0.00818845826415983\r",
      "Epoch: 3, loss: 0.05104506545028282\r",
      "Epoch: 3, loss: 0.0021538928127345\r",
      "Epoch: 3, loss: 0.004057145224344158\r",
      "Epoch: 3, loss: 0.003761547628654561\r",
      "Epoch: 3, loss: 0.00654907980867514\r",
      "Epoch: 3, loss: 0.006330600461430487\r",
      "Epoch: 3, loss: 0.009721395615069146\r",
      "Epoch: 3, loss: 0.049344778039969214\r",
      "Epoch: 3, loss: 0.0011363556264965186\r",
      "Epoch: 3, loss: 0.02669571217016904\r",
      "Epoch: 3, loss: 0.055577297616369835\r",
      "Epoch: 3, loss: 0.0035751038810006353\r",
      "Epoch: 3, loss: 0.00421488054766026\r",
      "Epoch: 3, loss: 0.08851366776301924\r",
      "Epoch: 3, loss: 0.08710161595659893\r",
      "Epoch: 3, loss: 0.024890942709833684\r",
      "Epoch: 3, loss: 0.004766699809613058\r",
      "Epoch: 3, loss: 0.022796818846939734\r",
      "Epoch: 3, loss: 0.010923913645603772\r",
      "Epoch: 3, loss: 0.03536087964058028\r",
      "Epoch: 3, loss: 0.07343419081817436\r",
      "Epoch: 3, loss: 0.037395434218315274\r",
      "Epoch: 3, loss: 0.031637478344911714\r",
      "Epoch: 3, loss: 0.008924668967718206\r",
      "Epoch: 3, loss: 0.044199721911121106\r",
      "Epoch: 3, loss: 0.0013829884086243368\r",
      "Epoch: 3, loss: 0.0432872878936941\r",
      "Epoch: 3, loss: 0.013253152061498345\r",
      "Epoch: 3, loss: 0.005221334065576309\r",
      "Epoch: 3, loss: 0.04833094342875312\r",
      "Epoch: 3, loss: 0.08417484350016631\r",
      "Epoch: 3, loss: 0.009678898446620086\r",
      "Epoch: 3, loss: 0.03060091556880365\r",
      "Epoch: 3, loss: 0.04653035661369785\r",
      "Epoch: 3, loss: 0.07951519639006667\r",
      "Epoch: 3, loss: 0.012377567798848287\r",
      "Epoch: 3, loss: 0.004641058527125258\r",
      "Epoch: 3, loss: 0.015650191366168853\r",
      "Epoch: 3, loss: 0.01954061339748216\r",
      "Epoch: 3, loss: 0.012278623692359006\r",
      "Epoch: 3, loss: 0.0022159505937065856\r",
      "Epoch: 3, loss: 0.012698309974484647\r",
      "Epoch: 3, loss: 0.008194859622649616\r",
      "Epoch: 3, loss: 0.004932905702033474\r",
      "Epoch: 3, loss: 0.006655434641466999\r",
      "Epoch: 3, loss: 0.0016126347707908963\r",
      "Epoch: 3, loss: 0.0009971355776814727\r",
      "Epoch: 3, loss: 0.03146176864261301\r",
      "Epoch: 3, loss: 0.0014240847046497378\r",
      "Epoch: 4, loss: 0.03482756848196478\r",
      "Epoch: 4, loss: 0.0018918784152082712\r",
      "Epoch: 4, loss: 0.021423196333986715\r",
      "Epoch: 4, loss: 0.029839458241547462\r",
      "Epoch: 4, loss: 0.012181685276151866\r",
      "Epoch: 4, loss: 0.0105737986394319\r",
      "Epoch: 4, loss: 0.007372339261981573\r",
      "Epoch: 4, loss: 0.013791463457459914\r",
      "Epoch: 4, loss: 0.0021871576064578777\r",
      "Epoch: 4, loss: 0.013927889417325168\r",
      "Epoch: 4, loss: 0.002081820457679037\r",
      "Epoch: 4, loss: 0.00260858908208\r",
      "Epoch: 4, loss: 0.01324908229462917\r",
      "Epoch: 4, loss: 0.004285965488930357\r",
      "Epoch: 4, loss: 0.014044864371968148\r",
      "Epoch: 4, loss: 0.006703281217049713\r",
      "Epoch: 4, loss: 0.019363380008310206\r",
      "Epoch: 4, loss: 0.010946439702198893\r",
      "Epoch: 4, loss: 0.01221945762319883\r",
      "Epoch: 4, loss: 0.007043564188265563\r",
      "Epoch: 4, loss: 0.0006875269856326063\r",
      "Epoch: 4, loss: 0.008321472842971866\r",
      "Epoch: 4, loss: 0.003894015117785323\r",
      "Epoch: 4, loss: 0.0038423372434894533\r",
      "Epoch: 4, loss: 0.003279621545374702\r",
      "Epoch: 4, loss: 0.0016683251466361477\r",
      "Epoch: 4, loss: 0.0015459916156731384\r",
      "Epoch: 4, loss: 0.0019733130308493066\r",
      "Epoch: 4, loss: 0.00029795380684933555\r",
      "Epoch: 4, loss: 0.00045193403371836575\r",
      "Epoch: 4, loss: 0.04383438713118157\r",
      "Epoch: 4, loss: 0.0045864873320676245\r",
      "Epoch: 4, loss: 0.004598517805800637\r",
      "Epoch: 4, loss: 0.0003821010127113934\r",
      "Epoch: 4, loss: 0.003617922787411577\r",
      "Epoch: 4, loss: 0.0033918712626388363\r",
      "Epoch: 4, loss: 0.0020867429887022346\r",
      "Epoch: 4, loss: 0.0004541176591647039\r",
      "Epoch: 4, loss: 0.002229039886581492\r",
      "Epoch: 4, loss: 0.004763489654899181\r",
      "Epoch: 4, loss: 0.0005133424622159291\r",
      "Epoch: 4, loss: 0.00010699893527623579\r",
      "Epoch: 4, loss: 0.021636707111231918\r",
      "Epoch: 4, loss: 0.02208957633821109\r",
      "Epoch: 4, loss: 0.0008241491593147914\r",
      "Epoch: 4, loss: 0.005975800360281675\r",
      "Epoch: 4, loss: 0.0006515545811386547\r",
      "Epoch: 4, loss: 0.009263767774344959\r",
      "Epoch: 4, loss: 0.0009814226719146635\r",
      "Epoch: 4, loss: 0.003370286097846305\r",
      "Epoch: 4, loss: 0.001367168203086618\r",
      "Epoch: 4, loss: 0.007264053267001491\r",
      "Epoch: 4, loss: 0.000505974542343534\r",
      "Epoch: 4, loss: 0.0003640676201675914\r",
      "Epoch: 4, loss: 0.0003192545336809072\r",
      "Epoch: 4, loss: 0.0009842977672505095\r",
      "Epoch: 4, loss: 0.0011981637186011412\r",
      "Epoch: 4, loss: 0.0009880921548545553\r",
      "Epoch: 4, loss: 0.00640694487456365\r",
      "Epoch: 4, loss: 0.00019507522699797062\r",
      "Epoch: 4, loss: 0.0026796461907592915\r",
      "Epoch: 4, loss: 0.007385946772637343\r",
      "Epoch: 4, loss: 0.0006564070998634773\r",
      "Epoch: 4, loss: 0.0003671648282555324\r",
      "Epoch: 4, loss: 0.010218106150407653\r",
      "Epoch: 4, loss: 0.011068738121323442\r",
      "Epoch: 4, loss: 0.0034211038857393313\r",
      "Epoch: 4, loss: 0.00038883917554951376\r",
      "Epoch: 4, loss: 0.0022922939156759877\r",
      "Epoch: 4, loss: 0.0007971700625260618\r",
      "Epoch: 4, loss: 0.003599607497756307\r",
      "Epoch: 4, loss: 0.008099647957149412\r",
      "Epoch: 4, loss: 0.0040866572969237855\r",
      "Epoch: 4, loss: 0.00461930695824203\r",
      "Epoch: 4, loss: 0.0007734845822616735\r",
      "Epoch: 4, loss: 0.006258679946627169\r",
      "Epoch: 4, loss: 0.00017691453345237856\r",
      "Epoch: 4, loss: 0.005421744025648942\r",
      "Epoch: 4, loss: 0.001859902889884049\r",
      "Epoch: 4, loss: 0.0008379707122728743\r",
      "Epoch: 4, loss: 0.0065440409967928\r",
      "Epoch: 4, loss: 0.010420948112655872\r",
      "Epoch: 4, loss: 0.0013497675870176157\r",
      "Epoch: 4, loss: 0.004349757390685754\r",
      "Epoch: 4, loss: 0.005824290080042998\r",
      "Epoch: 4, loss: 0.009899693641778425\r",
      "Epoch: 4, loss: 0.0012151954222892891\r",
      "Epoch: 4, loss: 0.0004917108091074021\r",
      "Epoch: 4, loss: 0.002345783904846733\r",
      "Epoch: 4, loss: 0.0023615947765792114\r",
      "Epoch: 4, loss: 0.0011707591290906637\r",
      "Epoch: 4, loss: 0.0003492224474003818\r",
      "Epoch: 4, loss: 0.0017579788092506104\r",
      "Epoch: 4, loss: 0.0008116415872741986\r",
      "Epoch: 4, loss: 0.0007580362058095444\r",
      "Epoch: 4, loss: 0.0010883274313208023\r",
      "Epoch: 4, loss: 0.00026280622269392545\r",
      "Epoch: 4, loss: 0.00017620084546631312\r",
      "Epoch: 4, loss: 0.004344671718881435\r",
      "Epoch: 4, loss: 0.00013600488797620253\r",
      "Epoch: 5, loss: 0.004312407904833146\r",
      "Epoch: 5, loss: 0.0003085294958616914\r",
      "Epoch: 5, loss: 0.0029829515009856537\r",
      "Epoch: 5, loss: 0.004043356707279423\r",
      "Epoch: 5, loss: 0.0016973668951156167\r",
      "Epoch: 5, loss: 0.0009199788092596608\r",
      "Epoch: 5, loss: 0.0008224118053214274\r",
      "Epoch: 5, loss: 0.0017715846360986283\r",
      "Epoch: 5, loss: 0.00031910643608175647\r",
      "Epoch: 5, loss: 0.0017815208486043736\r",
      "Epoch: 5, loss: 0.00018392151062134683\r",
      "Epoch: 5, loss: 0.00022601694609203495\r",
      "Epoch: 5, loss: 0.001427215110270677\r",
      "Epoch: 5, loss: 0.0007040755522537176\r",
      "Epoch: 5, loss: 0.0018216413335447046\r",
      "Epoch: 5, loss: 0.0006752990069287966\r",
      "Epoch: 5, loss: 0.0026024221254312124\r",
      "Epoch: 5, loss: 0.0014539230289071128\r",
      "Epoch: 5, loss: 0.0016903450617340547\r",
      "Epoch: 5, loss: 0.0009343036727461778\r",
      "Epoch: 5, loss: 0.00013511083724209843\r",
      "Epoch: 5, loss: 0.001136121099559082\r",
      "Epoch: 5, loss: 0.00045966318676125747\r",
      "Epoch: 5, loss: 0.0006763775118206561\r",
      "Epoch: 5, loss: 0.0004717313672058382\r",
      "Epoch: 5, loss: 0.00032670184810422215\r",
      "Epoch: 5, loss: 0.00013835489929269992\r",
      "Epoch: 5, loss: 0.0002529343114116691\r",
      "Epoch: 5, loss: 7.023266030649751e-05\r",
      "Epoch: 5, loss: 5.2591199498995786e-05\r",
      "Epoch: 5, loss: 0.004933885529402969\r",
      "Epoch: 5, loss: 0.000520265474578581\r",
      "Epoch: 5, loss: 0.0006983468376050246\r",
      "Epoch: 5, loss: 4.541745726422596e-05\r",
      "Epoch: 5, loss: 0.000589438489729447\r",
      "Epoch: 5, loss: 0.00028431980278520685\r",
      "Epoch: 5, loss: 0.00030651912294820684\r",
      "Epoch: 5, loss: 4.860709056275261e-05\r",
      "Epoch: 5, loss: 0.00033077551821484186\r",
      "Epoch: 5, loss: 0.00043415220946648814\r",
      "Epoch: 5, loss: 0.00010165166061475174\r",
      "Epoch: 5, loss: 2.0016064328448424e-05\r",
      "Epoch: 5, loss: 0.0026737826416959867\r",
      "Epoch: 5, loss: 0.002893058830214063\r",
      "Epoch: 5, loss: 8.874701466708823e-05\r",
      "Epoch: 5, loss: 0.0007184827044859323\r",
      "Epoch: 5, loss: 8.292600143842356e-05\r",
      "Epoch: 5, loss: 0.0012558123310574924\r",
      "Epoch: 5, loss: 8.08732615009151e-05\r",
      "Epoch: 5, loss: 0.0004946162549237568\r",
      "Epoch: 5, loss: 0.00021654176270853159\r",
      "Epoch: 5, loss: 0.0010156725375363971\r",
      "Epoch: 5, loss: 9.718293914182864e-05\r",
      "Epoch: 5, loss: 3.49281593291554e-05\r",
      "Epoch: 5, loss: 2.8769626307969864e-05\r",
      "Epoch: 5, loss: 0.0001486454248181439\r",
      "Epoch: 5, loss: 0.00020661011254819798\r",
      "Epoch: 5, loss: 0.00011557594179687133\r",
      "Epoch: 5, loss: 0.0008404192583861186\r",
      "Epoch: 5, loss: 3.549855727854092e-05\r",
      "Epoch: 5, loss: 0.0002709303141748628\r",
      "Epoch: 5, loss: 0.0009809220124161419\r",
      "Epoch: 5, loss: 0.00011082717448911435\r",
      "Epoch: 5, loss: 3.20849423020014e-05\r",
      "Epoch: 5, loss: 0.0012311071447438184\r",
      "Epoch: 5, loss: 0.001417352689479844\r",
      "Epoch: 5, loss: 0.0004691124160114806\r",
      "Epoch: 5, loss: 3.190755308345797e-05\r",
      "Epoch: 5, loss: 0.000232667159515403\r",
      "Epoch: 5, loss: 6.34322848171533e-05\r",
      "Epoch: 5, loss: 0.00036546836646046926\r",
      "Epoch: 5, loss: 0.0008921326697275679\r",
      "Epoch: 5, loss: 0.00044490493324359013\r",
      "Epoch: 5, loss: 0.0006595114009111789\r",
      "Epoch: 5, loss: 6.909522703520005e-05\r",
      "Epoch: 5, loss: 0.0008716194824351195\r",
      "Epoch: 5, loss: 2.5984883317155044e-05\r",
      "Epoch: 5, loss: 0.0006938556506421419\r",
      "Epoch: 5, loss: 0.00026065724659547014\r",
      "Epoch: 5, loss: 0.00012943341380925537\r",
      "Epoch: 5, loss: 0.0008796015064895821\r",
      "Epoch: 5, loss: 0.001310191942955935\r",
      "Epoch: 5, loss: 0.00018950986418513505\r",
      "Epoch: 5, loss: 0.0006076429847788775\r",
      "Epoch: 5, loss: 0.0007415788214128866\r",
      "Epoch: 5, loss: 0.0012467220285024686\r",
      "Epoch: 5, loss: 0.00012033789036342017\r",
      "Epoch: 5, loss: 5.934896333752625e-05\r",
      "Epoch: 5, loss: 0.0003416377454993801\r",
      "Epoch: 5, loss: 0.0002976655528939268\r",
      "Epoch: 5, loss: 0.00011064701798604477\r",
      "Epoch: 5, loss: 5.48073288672972e-05\r",
      "Epoch: 5, loss: 0.0002430111593981927\r",
      "Epoch: 5, loss: 7.897551747781075e-05\r",
      "Epoch: 5, loss: 0.00011366680369873954\r",
      "Epoch: 5, loss: 0.00016893221666984868\r",
      "Epoch: 5, loss: 4.232123738742019e-05\r",
      "Epoch: 5, loss: 3.01857240062144e-05\r",
      "Epoch: 5, loss: 0.0005927038433409671\r",
      "Epoch: 5, loss: 1.4907818086782265e-05\r",
      "Epoch: 6, loss: 0.000545375493351234\r",
      "Epoch: 6, loss: 4.906592038182391e-05\r",
      "Epoch: 6, loss: 0.0004099458687770051\r",
      "Epoch: 6, loss: 0.0005432561176304217\r",
      "Epoch: 6, loss: 0.00023453417355154645\r",
      "Epoch: 6, loss: 9.172685032496873e-05\r",
      "Epoch: 6, loss: 0.00010021964792200999\r",
      "Epoch: 6, loss: 0.00023189751706778088\r",
      "Epoch: 6, loss: 4.7502271435314926e-05\r",
      "Epoch: 6, loss: 0.0002322729838568138\r",
      "Epoch: 6, loss: 1.603268371169254e-05\r",
      "Epoch: 6, loss: 1.9736644015721572e-05\r",
      "Epoch: 6, loss: 0.00015407063667335886\r",
      "Epoch: 6, loss: 0.00010953776815138654\r",
      "Epoch: 6, loss: 0.0002393160036648481\r",
      "Epoch: 6, loss: 7.585564508675269e-05\r",
      "Epoch: 6, loss: 0.00034853528192041835\r",
      "Epoch: 6, loss: 0.0001944653614977504\r",
      "Epoch: 6, loss: 0.00023187246482268787\r",
      "Epoch: 6, loss: 0.0001258007694109789\r",
      "Epoch: 6, loss: 2.41029958179375e-05\r",
      "Epoch: 6, loss: 0.00015515851453813266\r",
      "Epoch: 6, loss: 5.840108844457794e-05\r",
      "Epoch: 6, loss: 0.00011273813362126031\r",
      "Epoch: 6, loss: 6.78097670391798e-05\r",
      "Epoch: 6, loss: 5.7304155392450405e-05\r",
      "Epoch: 6, loss: 1.221825706245105e-05\r",
      "Epoch: 6, loss: 3.4640644958293905e-05\r",
      "Epoch: 6, loss: 1.3950807876370561e-05\r",
      "Epoch: 6, loss: 7.317207648614602e-06\r",
      "Epoch: 6, loss: 0.0005773975097140573\r",
      "Epoch: 6, loss: 6.35934151991142e-05\r",
      "Epoch: 6, loss: 0.00010282816104382625\r",
      "Epoch: 6, loss: 6.538705810638508e-06\r",
      "Epoch: 6, loss: 9.123128952882975e-05\r",
      "Epoch: 6, loss: 2.3761525239750288e-05\r",
      "Epoch: 6, loss: 4.486264442955135e-05\r",
      "Epoch: 6, loss: 6.1719896158148445e-06\r",
      "Epoch: 6, loss: 4.852965868026231e-05\r",
      "Epoch: 6, loss: 3.96290735031383e-05\r",
      "Epoch: 6, loss: 1.7981913342413594e-05\r",
      "Epoch: 6, loss: 4.33788028490516e-06\r",
      "Epoch: 6, loss: 0.00033629321427605245\r",
      "Epoch: 6, loss: 0.00037800178465552164\r",
      "Epoch: 6, loss: 1.1065720865557368e-05\r",
      "Epoch: 6, loss: 9.046984738752788e-05\r",
      "Epoch: 6, loss: 1.1696536317093544e-05\r",
      "Epoch: 6, loss: 0.00016925795220758285\r",
      "Epoch: 6, loss: 7.309690655849813e-06\r",
      "Epoch: 6, loss: 7.113646206179149e-05\r",
      "Epoch: 6, loss: 3.315326207147893e-05\r",
      "Epoch: 6, loss: 0.0001401229227443196\r",
      "Epoch: 6, loss: 1.6839582998797292e-05\r",
      "Epoch: 6, loss: 3.86757314828212e-06\r",
      "Epoch: 6, loss: 3.0447256952468487e-06\r",
      "Epoch: 6, loss: 2.2359042684100694e-05\r",
      "Epoch: 6, loss: 3.3208225060514466e-05\r",
      "Epoch: 6, loss: 1.50623558286042e-05\r",
      "Epoch: 6, loss: 0.00011106180202690821\r",
      "Epoch: 6, loss: 6.349129570009849e-06\r",
      "Epoch: 6, loss: 2.7257899177113725e-05\r",
      "Epoch: 6, loss: 0.00013033707962130066\r",
      "Epoch: 6, loss: 1.7779973800896874e-05\r",
      "Epoch: 6, loss: 2.976199334671921e-06\r",
      "Epoch: 6, loss: 0.00015282712852837722\r",
      "Epoch: 6, loss: 0.0001826906910011325\r",
      "Epoch: 6, loss: 6.422932038911241e-05\r",
      "Epoch: 6, loss: 2.781316722778844e-06\r",
      "Epoch: 6, loss: 2.3532861233523253e-05\r",
      "Epoch: 6, loss: 5.647619477932584e-06\r",
      "Epoch: 6, loss: 3.6910929365494526e-05\r",
      "Epoch: 6, loss: 9.81856123510126e-05\r",
      "Epoch: 6, loss: 4.8227035958166496e-05\r",
      "Epoch: 6, loss: 9.250177676126143e-05\r",
      "Epoch: 6, loss: 6.217706426180909e-06\r",
      "Epoch: 6, loss: 0.00011983910413732875\r",
      "Epoch: 6, loss: 4.11580756297805e-06\r",
      "Epoch: 6, loss: 9.013297460310393e-05\r",
      "Epoch: 6, loss: 3.6460897553296226e-05\r",
      "Epoch: 6, loss: 1.9484963589106708e-05\r",
      "Epoch: 6, loss: 0.00011771548378080436\r",
      "Epoch: 6, loss: 0.00016669009257658037\r",
      "Epoch: 6, loss: 2.666896239849555e-05\r",
      "Epoch: 6, loss: 8.376396902846063e-05\r",
      "Epoch: 6, loss: 9.56062729771074e-05\r",
      "Epoch: 6, loss: 0.0001584837496439569\r",
      "Epoch: 6, loss: 1.1852907040160546e-05\r",
      "Epoch: 6, loss: 7.869718334989589e-06\r",
      "Epoch: 6, loss: 4.8700869342864424e-05\r",
      "Epoch: 6, loss: 3.857087752875111e-05\r",
      "Epoch: 6, loss: 1.0306182969336279e-05\r",
      "Epoch: 6, loss: 8.502394128684153e-06\r",
      "Epoch: 6, loss: 3.354692321913005e-05\r",
      "Epoch: 6, loss: 7.527636035689213e-06\r",
      "Epoch: 6, loss: 1.675087598713186e-05\r",
      "Epoch: 6, loss: 2.5264798082535565e-05\r",
      "Epoch: 6, loss: 6.696258326220966e-06\r",
      "Epoch: 6, loss: 5.00356005815677e-06\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, loss: 8.015660878887483e-05\r",
      "Epoch: 6, loss: 1.92169990982142e-06\r",
      "Epoch: 7, loss: 7.002199334435447e-05\r",
      "Epoch: 7, loss: 7.6341850366435e-06\r",
      "Epoch: 7, loss: 5.5837573662088584e-05\r",
      "Epoch: 7, loss: 7.259696219541375e-05\r",
      "Epoch: 7, loss: 3.223313931209298e-05\r",
      "Epoch: 7, loss: 1.0353117793952509e-05\r",
      "Epoch: 7, loss: 1.2960771632997722e-05\r",
      "Epoch: 7, loss: 3.0735810111535874e-05\r",
      "Epoch: 7, loss: 7.103317180859528e-06\r",
      "Epoch: 7, loss: 3.067346825982929e-05\r",
      "Epoch: 7, loss: 1.425729621555395e-06\r",
      "Epoch: 7, loss: 1.739247299402815e-06\r",
      "Epoch: 7, loss: 1.6577625465111674e-05\r",
      "Epoch: 7, loss: 1.6434689262121017e-05\r",
      "Epoch: 7, loss: 3.1719979775599945e-05\r",
      "Epoch: 7, loss: 9.244330287320597e-06\r",
      "Epoch: 7, loss: 4.6604675661529685e-05\r",
      "Epoch: 7, loss: 2.6135730732523515e-05\r",
      "Epoch: 7, loss: 3.1640527578827185e-05\r",
      "Epoch: 7, loss: 1.7091379379299087e-05\r",
      "Epoch: 7, loss: 4.0450536992118034e-06\r",
      "Epoch: 7, loss: 2.1193217629558673e-05\r",
      "Epoch: 7, loss: 7.773254746972035e-06\r",
      "Epoch: 7, loss: 1.774114556439935e-05\r",
      "Epoch: 7, loss: 9.719094149679487e-06\r",
      "Epoch: 7, loss: 9.310952445934373e-06\r",
      "Epoch: 7, loss: 1.093754881988162e-06\r",
      "Epoch: 7, loss: 4.918908916507131e-06\r",
      "Epoch: 7, loss: 2.50779002652475e-06\r",
      "Epoch: 7, loss: 1.150430494821326e-06\r",
      "Epoch: 7, loss: 6.950121118501107e-05\r",
      "Epoch: 7, loss: 8.170693015959182e-06\r",
      "Epoch: 7, loss: 1.4828692283695463e-05\r",
      "Epoch: 7, loss: 1.0600464689496133e-06\r",
      "Epoch: 7, loss: 1.361813477181186e-05\r",
      "Epoch: 7, loss: 1.966948025319874e-06\r",
      "Epoch: 7, loss: 6.529874629396131e-06\r",
      "Epoch: 7, loss: 9.096830993594512e-07\r",
      "Epoch: 7, loss: 7.052309186694199e-06\r",
      "Epoch: 7, loss: 3.587534455967042e-06\r",
      "Epoch: 7, loss: 2.978592893598427e-06\r",
      "Epoch: 7, loss: 8.736516804546352e-07\r",
      "Epoch: 7, loss: 4.285804899945444e-05\r",
      "Epoch: 7, loss: 4.9365231022340925e-05\r",
      "Epoch: 7, loss: 1.5392675919316923e-06\r",
      "Epoch: 7, loss: 1.174104884210186e-05\r",
      "Epoch: 7, loss: 1.7451277983646687e-06\r",
      "Epoch: 7, loss: 2.273913835111275e-05\r",
      "Epoch: 7, loss: 7.524758217225917e-07\r",
      "Epoch: 7, loss: 1.0092728866412088e-05\r",
      "Epoch: 7, loss: 4.9598684351491054e-06\r",
      "Epoch: 7, loss: 1.9157986098158935e-05\r",
      "Epoch: 7, loss: 2.7465833795862913e-06\r",
      "Epoch: 7, loss: 5.156381300085718e-07\r",
      "Epoch: 7, loss: 4.0577220853883787e-07\r",
      "Epoch: 7, loss: 3.337384090675725e-06\r",
      "Epoch: 7, loss: 5.096435718617335e-06\r",
      "Epoch: 7, loss: 2.101034096119829e-06\r",
      "Epoch: 7, loss: 1.475819446328649e-05\r",
      "Epoch: 7, loss: 1.0953020656769245e-06\r",
      "Epoch: 7, loss: 2.709985763609438e-06\r",
      "Epoch: 7, loss: 1.733841161259033e-05\r",
      "Epoch: 7, loss: 2.7577162344645007e-06\r",
      "Epoch: 7, loss: 3.212825351862202e-07\r",
      "Epoch: 7, loss: 1.937356250258064e-05\r",
      "Epoch: 7, loss: 2.368442747801443e-05\r",
      "Epoch: 7, loss: 8.785538096328502e-06\r",
      "Epoch: 7, loss: 2.871789504433264e-07\r",
      "Epoch: 7, loss: 2.354815800425237e-06\r",
      "Epoch: 7, loss: 5.799042643020481e-07\r",
      "Epoch: 7, loss: 3.6993072553293493e-06\r",
      "Epoch: 7, loss: 1.0803131084659462e-05\r",
      "Epoch: 7, loss: 5.205646508017064e-06\r",
      "Epoch: 7, loss: 1.2815092125696806e-05\r",
      "Epoch: 7, loss: 5.559068454307731e-07\r",
      "Epoch: 7, loss: 1.6334974343406328e-05\r",
      "Epoch: 7, loss: 6.67765213488167e-07\r",
      "Epoch: 7, loss: 1.1832330061643376e-05\r",
      "Epoch: 7, loss: 5.089528185009149e-06\r",
      "Epoch: 7, loss: 2.8804736672839395e-06\r",
      "Epoch: 7, loss: 1.5719996595204635e-05\r",
      "Epoch: 7, loss: 2.1406653748277465e-05\r",
      "Epoch: 7, loss: 3.7524228926431973e-06\r",
      "Epoch: 7, loss: 1.1443965531147153e-05\r",
      "Epoch: 7, loss: 1.244148917727711e-05\r",
      "Epoch: 7, loss: 2.0305215477210445e-05\r",
      "Epoch: 7, loss: 1.1515595645673623e-06\r",
      "Epoch: 7, loss: 1.1040606992810648e-06\r",
      "Epoch: 7, loss: 6.8411998030919895e-06\r",
      "Epoch: 7, loss: 5.088430699035864e-06\r",
      "Epoch: 7, loss: 9.415206001895416e-07\r",
      "Epoch: 7, loss: 1.3009743578679523e-06\r",
      "Epoch: 7, loss: 4.625474894995696e-06\r",
      "Epoch: 7, loss: 7.009220179598715e-07\r",
      "Epoch: 7, loss: 2.43706486322806e-06\r",
      "Epoch: 7, loss: 3.684969468613928e-06\r",
      "Epoch: 7, loss: 1.0404129900726129e-06\r",
      "Epoch: 7, loss: 8.054057732967992e-07\r",
      "Epoch: 7, loss: 1.0781662967498944e-05\r",
      "Epoch: 7, loss: 2.820977665072374e-07\r",
      "Epoch: 8, loss: 9.090118020594353e-06\r",
      "Epoch: 8, loss: 1.165952283241069e-06\r",
      "Epoch: 8, loss: 7.563317956713037e-06\r",
      "Epoch: 8, loss: 9.673468577345496e-06\r",
      "Epoch: 8, loss: 4.414998301055445e-06\r",
      "Epoch: 8, loss: 1.2846264507259822e-06\r",
      "Epoch: 8, loss: 1.7385580336007565e-06\r",
      "Epoch: 8, loss: 4.107747066620468e-06\r",
      "Epoch: 8, loss: 1.0587569766123694e-06\r",
      "Epoch: 8, loss: 4.085869115182488e-06\r",
      "Epoch: 8, loss: 1.3899017953081915e-07\r",
      "Epoch: 8, loss: 1.5872180180625848e-07\r",
      "Epoch: 8, loss: 1.7739829039236693e-06\r",
      "Epoch: 8, loss: 2.4071977698884074e-06\r",
      "Epoch: 8, loss: 4.230688817339221e-06\r",
      "Epoch: 8, loss: 1.1898831487372961e-06\r",
      "Epoch: 8, loss: 6.229997290041711e-06\r",
      "Epoch: 8, loss: 3.5244926468444348e-06\r",
      "Epoch: 8, loss: 4.30416932450213e-06\r",
      "Epoch: 8, loss: 2.3342586323487405e-06\r",
      "Epoch: 8, loss: 6.512521774345618e-07\r",
      "Epoch: 8, loss: 2.895032904364818e-06\r",
      "Epoch: 8, loss: 1.063040820980619e-06\r",
      "Epoch: 8, loss: 2.679196334334175e-06\r",
      "Epoch: 8, loss: 1.3876031192646746e-06\r",
      "Epoch: 8, loss: 1.4406131053260169e-06\r",
      "Epoch: 8, loss: 1.0542974932758433e-07\r",
      "Epoch: 8, loss: 7.101866603322351e-07\r",
      "Epoch: 8, loss: 4.232337614184933e-07\r",
      "Epoch: 8, loss: 1.9007333966364608e-07\r",
      "Epoch: 8, loss: 8.540895277244878e-06\r",
      "Epoch: 8, loss: 1.0831564747419684e-06\r",
      "Epoch: 8, loss: 2.1086257687704268e-06\r",
      "Epoch: 8, loss: 1.7880610478833912e-07\r",
      "Epoch: 8, loss: 1.9838501172945565e-06\r",
      "Epoch: 8, loss: 1.6276318867946866e-07\r",
      "Epoch: 8, loss: 9.446099820897415e-07\r",
      "Epoch: 8, loss: 1.4556952856382828e-07\r",
      "Epoch: 8, loss: 1.0165842060095912e-06\r",
      "Epoch: 8, loss: 3.196795094377724e-07\r",
      "Epoch: 8, loss: 4.729771171019624e-07\r",
      "Epoch: 8, loss: 1.6202070827073274e-07\r",
      "Epoch: 8, loss: 5.517464741032098e-06\r",
      "Epoch: 8, loss: 6.452675347096837e-06\r",
      "Epoch: 8, loss: 2.2790286787851522e-07\r",
      "Epoch: 8, loss: 1.5534763940810085e-06\r",
      "Epoch: 8, loss: 2.6625576862162886e-07\r",
      "Epoch: 8, loss: 3.0504902108842928e-06\r",
      "Epoch: 8, loss: 9.05799747301727e-08\r",
      "Epoch: 8, loss: 1.418756654868157e-06\r",
      "Epoch: 8, loss: 7.298443492016453e-07\r",
      "Epoch: 8, loss: 2.604625900461136e-06\r",
      "Epoch: 8, loss: 4.308522569421778e-07\r",
      "Epoch: 8, loss: 7.944567288724137e-08\r",
      "Epoch: 8, loss: 6.464935767381929e-08\r",
      "Epoch: 8, loss: 4.937390966409001e-07\r",
      "Epoch: 8, loss: 7.588550715875573e-07\r",
      "Epoch: 8, loss: 3.0367262327320836e-07\r",
      "Epoch: 8, loss: 1.9693971112298982e-06\r",
      "Epoch: 8, loss: 1.8230140362840627e-07\r",
      "Epoch: 8, loss: 2.65069530658301e-07\r",
      "Epoch: 8, loss: 2.3101083581466737e-06\r",
      "Epoch: 8, loss: 4.1769046639533455e-07\r",
      "Epoch: 8, loss: 4.2829605529984575e-08\r",
      "Epoch: 8, loss: 2.4928900933829015e-06\r",
      "Epoch: 8, loss: 3.0862775806378103e-06\r",
      "Epoch: 8, loss: 1.2009707879705907e-06\r",
      "Epoch: 8, loss: 3.812035782342968e-08\r",
      "Epoch: 8, loss: 2.3208628280066803e-07\r",
      "Epoch: 8, loss: 6.942487088808885e-08\r",
      "Epoch: 8, loss: 3.6703094910295757e-07\r",
      "Epoch: 8, loss: 1.1888570140265658e-06\r",
      "Epoch: 8, loss: 5.596592937117642e-07\r",
      "Epoch: 8, loss: 1.7611366100378442e-06\r",
      "Epoch: 8, loss: 4.923548677692839e-08\r",
      "Epoch: 8, loss: 2.214714044487177e-06\r",
      "Epoch: 8, loss: 1.0796761659496849e-07\r",
      "Epoch: 8, loss: 1.565120771829154e-06\r",
      "Epoch: 8, loss: 7.089503608380367e-07\r",
      "Epoch: 8, loss: 4.2015074590815883e-07\r",
      "Epoch: 8, loss: 2.0979929619907484e-06\r",
      "Epoch: 8, loss: 2.7700093880834356e-06\r",
      "Epoch: 8, loss: 5.272151223590084e-07\r",
      "Epoch: 8, loss: 1.5547864198721838e-06\r",
      "Epoch: 8, loss: 1.6307105797719776e-06\r",
      "Epoch: 8, loss: 2.6190603238015378e-06\r",
      "Epoch: 8, loss: 1.0975857495320081e-07\r",
      "Epoch: 8, loss: 1.593655186404086e-07\r",
      "Epoch: 8, loss: 9.516860498020966e-07\r",
      "Epoch: 8, loss: 6.791279766870525e-07\r",
      "Epoch: 8, loss: 8.407430304740785e-08\r",
      "Epoch: 8, loss: 1.963807208872696e-07\r",
      "Epoch: 8, loss: 6.370806191434722e-07\r",
      "Epoch: 8, loss: 6.363262303054813e-08\r",
      "Epoch: 8, loss: 3.510936504224385e-07\r",
      "Epoch: 8, loss: 5.284940208969512e-07\r",
      "Epoch: 8, loss: 1.589555180264914e-07\r",
      "Epoch: 8, loss: 1.2647110032466112e-07\r",
      "Epoch: 8, loss: 1.4459498501855582e-06\r",
      "Epoch: 8, loss: 4.440332098217156e-08\r",
      "Epoch: 9, loss: 1.189861808801368e-06\r",
      "Epoch: 9, loss: 1.7530897166074258e-07\r",
      "Epoch: 9, loss: 1.0212516893761214e-06\r",
      "Epoch: 9, loss: 1.2876124305156196e-06\r",
      "Epoch: 9, loss: 6.034680388552441e-07\r",
      "Epoch: 9, loss: 1.6968340776764737e-07\r",
      "Epoch: 9, loss: 2.3816425487113795e-07\r",
      "Epoch: 9, loss: 5.520749501450242e-07\r",
      "Epoch: 9, loss: 1.5676002304923746e-07\r",
      "Epoch: 9, loss: 5.474992548133749e-07\r",
      "Epoch: 9, loss: 1.6252043475757875e-08\r",
      "Epoch: 9, loss: 1.588712294413155e-08\r",
      "Epoch: 9, loss: 1.8864911814228207e-07\r",
      "Epoch: 9, loss: 3.468984591063805e-07\r",
      "Epoch: 9, loss: 5.668221477961221e-07\r",
      "Epoch: 9, loss: 1.5845544305663253e-07\r",
      "Epoch: 9, loss: 8.332549683407361e-07\r",
      "Epoch: 9, loss: 4.764455721760321e-07\r",
      "Epoch: 9, loss: 5.845165270789005e-07\r",
      "Epoch: 9, loss: 3.197410964342748e-07\r",
      "Epoch: 9, loss: 1.017652044109026e-07\r",
      "Epoch: 9, loss: 3.954795294735943e-07\r",
      "Epoch: 9, loss: 1.4751950160764278e-07\r",
      "Epoch: 9, loss: 3.93552864757219e-07\r",
      "Epoch: 9, loss: 1.972798933336695e-07\r",
      "Epoch: 9, loss: 2.1593590733075092e-07\r",
      "Epoch: 9, loss: 1.1896644107241054e-08\r",
      "Epoch: 9, loss: 1.0309084870522898e-07\r",
      "Epoch: 9, loss: 6.843267667557446e-08\r",
      "Epoch: 9, loss: 3.154386563191785e-08\r",
      "Epoch: 9, loss: 1.066111093941255e-06\r",
      "Epoch: 9, loss: 1.463164691769459e-07\r",
      "Epoch: 9, loss: 2.9698907714983166e-07\r",
      "Epoch: 9, loss: 3.004888821454344e-08\r",
      "Epoch: 9, loss: 2.843066346060079e-07\r",
      "Epoch: 9, loss: 1.4081017687568673e-08\r",
      "Epoch: 9, loss: 1.3581700824559888e-07\r",
      "Epoch: 9, loss: 2.3894170927856166e-08\r",
      "Epoch: 9, loss: 1.4553353735945013e-07\r",
      "Epoch: 9, loss: 2.7919490695441625e-08\r",
      "Epoch: 9, loss: 7.294768132808963e-08\r",
      "Epoch: 9, loss: 2.8208971940578484e-08\r",
      "Epoch: 9, loss: 7.159950132387993e-07\r",
      "Epoch: 9, loss: 8.449861899160913e-07\r",
      "Epoch: 9, loss: 3.4648803610393007e-08\r",
      "Epoch: 9, loss: 2.0807705746447228e-07\r",
      "Epoch: 9, loss: 4.075446492540567e-08\r",
      "Epoch: 9, loss: 4.0911085469568385e-07\r",
      "Epoch: 9, loss: 1.2528331720052376e-08\r",
      "Epoch: 9, loss: 1.9816467236491794e-07\r",
      "Epoch: 9, loss: 1.0607963575293237e-07\r",
      "Epoch: 9, loss: 3.529584900169179e-07\r",
      "Epoch: 9, loss: 6.57954832672747e-08\r",
      "Epoch: 9, loss: 1.3092068377251411e-08\r",
      "Epoch: 9, loss: 1.1080706743160657e-08\r",
      "Epoch: 9, loss: 7.240139699200925e-08\r",
      "Epoch: 9, loss: 1.1073109279485432e-07\r",
      "Epoch: 9, loss: 4.454572692559345e-08\r",
      "Epoch: 9, loss: 2.636735373549899e-07\r",
      "Epoch: 9, loss: 2.9418092260843057e-08\r",
      "Epoch: 9, loss: 2.5420892770665676e-08\r",
      "Epoch: 9, loss: 3.083355834485571e-07\r",
      "Epoch: 9, loss: 6.216474821680405e-08\r",
      "Epoch: 9, loss: 6.732972527210799e-09\r",
      "Epoch: 9, loss: 3.242759790533711e-07\r",
      "Epoch: 9, loss: 4.0401604639405666e-07\r",
      "Epoch: 9, loss: 1.6410527882428741e-07\r",
      "Epoch: 9, loss: 6.142899608160166e-09\r",
      "Epoch: 9, loss: 2.2454539665853834e-08\r",
      "Epoch: 9, loss: 9.437076602346298e-09\r",
      "Epoch: 9, loss: 3.595797684628888e-08\r",
      "Epoch: 9, loss: 1.3090429705376072e-07\r",
      "Epoch: 9, loss: 5.994162336298433e-08\r",
      "Epoch: 9, loss: 2.4080693923505774e-07\r",
      "Epoch: 9, loss: 4.378003769826534e-09\r",
      "Epoch: 9, loss: 2.9936779871177954e-07\r",
      "Epoch: 9, loss: 1.720430131323536e-08\r",
      "Epoch: 9, loss: 2.081852129653121e-07\r",
      "Epoch: 9, loss: 9.855554888313872e-08\r",
      "Epoch: 9, loss: 6.065780264829285e-08\r",
      "Epoch: 9, loss: 2.8010249930953447e-07\r",
      "Epoch: 9, loss: 3.606992345466205e-07\r",
      "Epoch: 9, loss: 7.392092420657083e-08\r",
      "Epoch: 9, loss: 2.1055569738094363e-07\r",
      "Epoch: 9, loss: 2.1495123198310537e-07\r",
      "Epoch: 9, loss: 3.3979649827558086e-07\r",
      "Epoch: 9, loss: 1.0224043898341986e-08\r",
      "Epoch: 9, loss: 2.326589362801648e-08\r",
      "Epoch: 9, loss: 1.3154639143238234e-07\r",
      "Epoch: 9, loss: 9.13250777082586e-08\r",
      "Epoch: 9, loss: 7.3430491948077575e-09\r",
      "Epoch: 9, loss: 2.9276013194541294e-08\r",
      "Epoch: 9, loss: 8.7663406978201e-08\r",
      "Epoch: 9, loss: 5.638496492268062e-09\r",
      "Epoch: 9, loss: 5.018820259668809e-08\r",
      "Epoch: 9, loss: 7.493176940113246e-08\r",
      "Epoch: 9, loss: 2.39271147608487e-08\r",
      "Epoch: 9, loss: 1.945449850541363e-08\r",
      "Epoch: 9, loss: 1.9368053718899498e-07\r",
      "Epoch: 9, loss: 7.146521226534657e-09\r",
      "Epoch: 10, loss: 1.567410380020851e-07\r",
      "Epoch: 10, loss: 2.601381399509708e-08\r",
      "Epoch: 10, loss: 1.3768733306279531e-07\r",
      "Epoch: 10, loss: 1.7141736563400192e-07\r",
      "Epoch: 10, loss: 8.23829705648893e-08\r",
      "Epoch: 10, loss: 2.3255623347797792e-08\r",
      "Epoch: 10, loss: 3.2996184238712e-08\r",
      "Epoch: 10, loss: 7.448329650752639e-08\r",
      "Epoch: 10, loss: 2.3029107564094487e-08\r",
      "Epoch: 10, loss: 7.366851261023075e-08\r",
      "Epoch: 10, loss: 2.327721973416928e-09\r",
      "Epoch: 10, loss: 1.8645085495823426e-09\r",
      "Epoch: 10, loss: 1.9929828524707177e-08\r",
      "Epoch: 10, loss: 4.943052513847232e-08\r",
      "Epoch: 10, loss: 7.619499266644192e-08\r",
      "Epoch: 10, loss: 2.1529016604733772e-08\r",
      "Epoch: 10, loss: 1.1156091919647845e-07\r",
      "Epoch: 10, loss: 6.452158893823516e-08\r",
      "Epoch: 10, loss: 7.931530586327122e-08\r",
      "Epoch: 10, loss: 4.3864483559827355e-08\r",
      "Epoch: 10, loss: 1.55485580792859e-08\r",
      "Epoch: 10, loss: 5.4025100799943786e-08\r",
      "Epoch: 10, loss: 2.0617581975577905e-08\r",
      "Epoch: 10, loss: 5.674707161313004e-08\r",
      "Epoch: 10, loss: 2.7932387569292302e-08\r",
      "Epoch: 10, loss: 3.1687807806870516e-08\r",
      "Epoch: 10, loss: 1.6308200182574188e-09\r",
      "Epoch: 10, loss: 1.4956423302518294e-08\r",
      "Epoch: 10, loss: 1.0729346897541389e-08\r",
      "Epoch: 10, loss: 5.1577638637480575e-09\r",
      "Epoch: 10, loss: 1.347029585052262e-07\r",
      "Epoch: 10, loss: 1.9981679017228015e-08\r",
      "Epoch: 10, loss: 4.155189992401152e-08\r",
      "Epoch: 10, loss: 4.950148630840651e-09\r",
      "Epoch: 10, loss: 4.0291317321091786e-08\r",
      "Epoch: 10, loss: 1.4049551926144736e-09\r",
      "Epoch: 10, loss: 1.9416565356425205e-08\r",
      "Epoch: 10, loss: 3.904002362212116e-09\r",
      "Epoch: 10, loss: 2.0711845998986516e-08\r",
      "Epoch: 10, loss: 2.3968878045394916e-09\r",
      "Epoch: 10, loss: 1.101446653859057e-08\r",
      "Epoch: 10, loss: 4.687976468270535e-09\r",
      "Epoch: 10, loss: 9.351468521955072e-08\r",
      "Epoch: 10, loss: 1.1091604567334301e-07\r",
      "Epoch: 10, loss: 5.297099195371782e-09\r",
      "Epoch: 10, loss: 2.8086680871935182e-08\r",
      "Epoch: 10, loss: 6.2014430572950284e-09\r",
      "Epoch: 10, loss: 5.4891365007706344e-08\r",
      "Epoch: 10, loss: 1.8911325327445776e-09\r",
      "Epoch: 10, loss: 2.7553392317954507e-08\r",
      "Epoch: 10, loss: 1.5271944305231428e-08\r",
      "Epoch: 10, loss: 4.7749565879742446e-08\r",
      "Epoch: 10, loss: 9.853310341479341e-09\r",
      "Epoch: 10, loss: 2.1846969261807176e-09\r",
      "Epoch: 10, loss: 1.9094677068257605e-09\r",
      "Epoch: 10, loss: 1.0529540846093816e-08\r",
      "Epoch: 10, loss: 1.593406784947118e-08\r",
      "Epoch: 10, loss: 6.55562529981546e-09\r",
      "Epoch: 10, loss: 3.539552595438481e-08\r",
      "Epoch: 10, loss: 4.626894068393483e-09\r",
      "Epoch: 10, loss: 2.383208482173402e-09\r",
      "Epoch: 10, loss: 4.122988752752539e-08\r",
      "Epoch: 10, loss: 9.128265644905514e-09\r",
      "Epoch: 10, loss: 1.1368476767428682e-09\r",
      "Epoch: 10, loss: 4.252485805161859e-08\r",
      "Epoch: 10, loss: 5.3107349998641775e-08\r",
      "Epoch: 10, loss: 2.2418222556986328e-08\r",
      "Epoch: 10, loss: 1.0673239989920507e-09\r",
      "Epoch: 10, loss: 2.126322148985172e-09\r",
      "Epoch: 10, loss: 1.3881652788643861e-09\r",
      "Epoch: 10, loss: 3.468810776442773e-09\r",
      "Epoch: 10, loss: 1.4426684115642172e-08\r",
      "Epoch: 10, loss: 6.3964006692524395e-09\r",
      "Epoch: 10, loss: 3.282651894554222e-08\r",
      "Epoch: 10, loss: 4.0719461194271807e-10\r",
      "Epoch: 10, loss: 4.0406834975181686e-08\r",
      "Epoch: 10, loss: 2.6936376099526147e-09\r",
      "Epoch: 10, loss: 2.7808814804286743e-08\r",
      "Epoch: 10, loss: 1.3675191922908982e-08\r",
      "Epoch: 10, loss: 8.686536930199079e-09\r",
      "Epoch: 10, loss: 3.743290506333374e-08\r",
      "Epoch: 10, loss: 4.7219372791468116e-08\r",
      "Epoch: 10, loss: 1.0340712179545073e-08\r",
      "Epoch: 10, loss: 2.8467568234973307e-08\r",
      "Epoch: 10, loss: 2.8463476154489493e-08\r",
      "Epoch: 10, loss: 4.431251511961822e-08\r",
      "Epoch: 10, loss: 9.287358219973296e-10\r",
      "Epoch: 10, loss: 3.4031848442822734e-09\r",
      "Epoch: 10, loss: 1.810730183354026e-08\r",
      "Epoch: 10, loss: 1.2341250433742591e-08\r",
      "Epoch: 10, loss: 6.346543852947631e-10\r",
      "Epoch: 10, loss: 4.316280476863814e-09\r",
      "Epoch: 10, loss: 1.2052417789346185e-08\r",
      "Epoch: 10, loss: 4.925920672277647e-10\r",
      "Epoch: 10, loss: 7.129266748503906e-09\r",
      "Epoch: 10, loss: 1.0539639160517926e-08\r",
      "Epoch: 10, loss: 3.555575615611904e-09\r",
      "Epoch: 10, loss: 2.941837366650731e-09\r",
      "Epoch: 10, loss: 2.5940067678394803e-08\r",
      "Epoch: 10, loss: 1.1466528736129538e-09\r"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(model1.parameters(), lr=0.01, lr_schedulers=None, momentum=0, dampening=0, \n",
    "                weight_decay=0, nesterov=False, maximize=False)\n",
    "criterion = MSELoss()\n",
    "\n",
    "EPOCHS = 10\n",
    "total_loss_1 = []\n",
    "for epoch in range(EPOCHS):\n",
    "    for input, target in zip(inputs, targets):\n",
    "        output = model1(input)\n",
    "        loss = criterion(target, output)\n",
    "        model1.backward(criterion.backward())\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"Epoch: {epoch+1}, loss: {loss.item()}\", end='\\r')\n",
    "        total_loss_1.append(loss.item())\n",
    "    optimizer.schedule_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "id": "d92c55cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 1.930481964944421e-08\r",
      "Epoch: 1, loss: 3.5419437564270906e-09\r",
      "Epoch: 1, loss: 1.7263113232092513e-08\r",
      "Epoch: 1, loss: 2.1261277421268762e-08\r",
      "Epoch: 1, loss: 1.0448368506989274e-08\r",
      "Epoch: 1, loss: 3.013923188413362e-09\r",
      "Epoch: 1, loss: 4.2654128960070715e-09\r",
      "Epoch: 1, loss: 9.365957552930239e-09\r",
      "Epoch: 1, loss: 3.1142597688119064e-09\r",
      "Epoch: 1, loss: 9.242109724997067e-09\r",
      "Epoch: 1, loss: 3.599767900416314e-10\r",
      "Epoch: 1, loss: 2.5005423655825486e-10\r",
      "Epoch: 1, loss: 1.9897558315606832e-09\r",
      "Epoch: 1, loss: 6.490333638232607e-09\r",
      "Epoch: 1, loss: 9.54728643764491e-09\r",
      "Epoch: 1, loss: 2.7448443792253783e-09\r",
      "Epoch: 1, loss: 1.3913869100922996e-08\r",
      "Epoch: 1, loss: 8.133942230967128e-09\r",
      "Epoch: 1, loss: 1.0006035373937491e-08\r",
      "Epoch: 1, loss: 5.594902904794131e-09\r",
      "Epoch: 1, loss: 2.165773818593087e-09\r",
      "Epoch: 1, loss: 6.85967854585735e-09\r",
      "Epoch: 1, loss: 2.681584969259324e-09\r",
      "Epoch: 1, loss: 7.50779047832677e-09\r",
      "Epoch: 1, loss: 3.6578558883312083e-09\r",
      "Epoch: 1, loss: 4.25535276598878e-09\r",
      "Epoch: 1, loss: 2.4482530961450376e-10\r",
      "Epoch: 1, loss: 2.0060671446818803e-09\r",
      "Epoch: 1, loss: 1.5249735926252713e-09\r",
      "Epoch: 1, loss: 7.67315782421484e-10\r",
      "Epoch: 1, loss: 1.600615040016955e-08\r",
      "Epoch: 1, loss: 2.5487144556962446e-09\r",
      "Epoch: 1, loss: 5.376501764439172e-09\r",
      "Epoch: 1, loss: 7.398109834115459e-10\r",
      "Epoch: 1, loss: 5.2643002803125784e-09\r",
      "Epoch: 1, loss: 1.7495430683901373e-10\r",
      "Epoch: 1, loss: 2.563414530204434e-09\r",
      "Epoch: 1, loss: 5.830650734881043e-10\r",
      "Epoch: 1, loss: 2.72291354016948e-09\r",
      "Epoch: 1, loss: 2.049780118180802e-10\r",
      "Epoch: 1, loss: 1.5184150243908996e-09\r",
      "Epoch: 1, loss: 6.98372734825708e-10\r",
      "Epoch: 1, loss: 1.1428272448956386e-08\r",
      "Epoch: 1, loss: 1.3594143116941665e-08\r",
      "Epoch: 1, loss: 7.477636201949545e-10\r",
      "Epoch: 1, loss: 3.5394131500040508e-09\r",
      "Epoch: 1, loss: 8.669989097452373e-10\r",
      "Epoch: 1, loss: 6.857710369626675e-09\r",
      "Epoch: 1, loss: 2.7655182726310614e-10\r",
      "Epoch: 1, loss: 3.5483315230066233e-09\r",
      "Epoch: 1, loss: 2.025764951454007e-09\r",
      "Epoch: 1, loss: 6.005011856178586e-09\r",
      "Epoch: 1, loss: 1.3491332568162475e-09\r",
      "Epoch: 1, loss: 3.351566690522176e-10\r",
      "Epoch: 1, loss: 3.002748345511448e-10\r",
      "Epoch: 1, loss: 1.4105300864575077e-09\r",
      "Epoch: 1, loss: 2.1085460088650113e-09\r",
      "Epoch: 1, loss: 8.924760655135132e-10\r",
      "Epoch: 1, loss: 4.428225309414411e-09\r",
      "Epoch: 1, loss: 6.61015748498331e-10\r",
      "Epoch: 1, loss: 2.1252833866089115e-10\r",
      "Epoch: 1, loss: 5.1380701849774954e-09\r",
      "Epoch: 1, loss: 1.2307997209838051e-09\r",
      "Epoch: 1, loss: 1.8113549243551513e-10\r",
      "Epoch: 1, loss: 5.219826311623213e-09\r",
      "Epoch: 1, loss: 6.522012437219916e-09\r",
      "Epoch: 1, loss: 2.846210265401698e-09\r",
      "Epoch: 1, loss: 1.7409265154211905e-10\r",
      "Epoch: 1, loss: 1.9121075228331856e-10\r",
      "Epoch: 1, loss: 1.9706130874736629e-10\r",
      "Epoch: 1, loss: 3.16099889691082e-10\r",
      "Epoch: 1, loss: 1.5017871634628843e-09\r",
      "Epoch: 1, loss: 6.44739616551223e-10\r",
      "Epoch: 1, loss: 4.154606822256858e-09\r",
      "Epoch: 1, loss: 4.226309689942859e-11\r",
      "Epoch: 1, loss: 5.071464756437256e-09\r",
      "Epoch: 1, loss: 3.8458489384504175e-10\r",
      "Epoch: 1, loss: 3.4657269606936057e-09\r",
      "Epoch: 1, loss: 1.7598331707506368e-09\r",
      "Epoch: 1, loss: 1.1473858351034634e-09\r",
      "Epoch: 1, loss: 4.6607337233008935e-09\r",
      "Epoch: 1, loss: 5.780112567839392e-09\r",
      "Epoch: 1, loss: 1.3403901290802127e-09\r",
      "Epoch: 1, loss: 3.5781983338308985e-09\r",
      "Epoch: 1, loss: 3.519734059881124e-09\r",
      "Epoch: 1, loss: 5.40488924517105e-09\r",
      "Epoch: 1, loss: 8.117524235654943e-11\r",
      "Epoch: 1, loss: 4.6043839650868334e-10\r",
      "Epoch: 1, loss: 2.310932455512802e-09\r",
      "Epoch: 1, loss: 1.5548969750220524e-09\r",
      "Epoch: 1, loss: 5.626083350313476e-11\r",
      "Epoch: 1, loss: 5.847622463700674e-10\r",
      "Epoch: 1, loss: 1.538724411029413e-09\r",
      "Epoch: 1, loss: 4.372078452469326e-11\r",
      "Epoch: 1, loss: 9.354937068283346e-10\r",
      "Epoch: 1, loss: 1.3695814154449602e-09\r",
      "Epoch: 1, loss: 4.848134305142243e-10\r",
      "Epoch: 1, loss: 4.0685278774417276e-10\r",
      "Epoch: 1, loss: 3.234725262403912e-09\r",
      "Epoch: 1, loss: 1.6855398073945555e-10\r",
      "Epoch: 2, loss: 2.5660368555464294e-09\r",
      "Epoch: 2, loss: 5.149475398977434e-10\r",
      "Epoch: 2, loss: 2.3264086240773177e-09\r",
      "Epoch: 2, loss: 2.8363487497256995e-09\r",
      "Epoch: 2, loss: 1.4247601991561013e-09\r",
      "Epoch: 2, loss: 4.2538926709753746e-10\r",
      "Epoch: 2, loss: 5.955639649133072e-10\r",
      "Epoch: 2, loss: 1.2694428319273198e-09\r",
      "Epoch: 2, loss: 4.506456597613609e-10\r",
      "Epoch: 2, loss: 1.2499646616205835e-09\r",
      "Epoch: 2, loss: 6.047053529270891e-11\r",
      "Epoch: 2, loss: 3.862125617136518e-11\r",
      "Epoch: 2, loss: 2.0730432430649636e-10\r",
      "Epoch: 2, loss: 9.11970116187446e-10\r",
      "Epoch: 2, loss: 1.289128957546256e-09\r",
      "Epoch: 2, loss: 3.7952314836994467e-10\r",
      "Epoch: 2, loss: 1.8676706136249106e-09\r",
      "Epoch: 2, loss: 1.1041478648693233e-09\r",
      "Epoch: 2, loss: 1.357467918176578e-09\r",
      "Epoch: 2, loss: 7.682584801124449e-10\r",
      "Epoch: 2, loss: 3.2068301747228394e-10\r",
      "Epoch: 2, loss: 9.370441118990655e-10\r",
      "Epoch: 2, loss: 3.760197266832648e-10\r",
      "Epoch: 2, loss: 1.059511293225559e-09\r",
      "Epoch: 2, loss: 5.141298569992525e-10\r",
      "Epoch: 2, loss: 6.088975689188155e-10\r",
      "Epoch: 2, loss: 4.045119055704171e-11\r",
      "Epoch: 2, loss: 2.8873516933123406e-10\r",
      "Epoch: 2, loss: 2.294634909219087e-10\r",
      "Epoch: 2, loss: 1.203072111414986e-10\r",
      "Epoch: 2, loss: 2.058399060429481e-09\r",
      "Epoch: 2, loss: 3.5134349230512684e-10\r",
      "Epoch: 2, loss: 7.460640422256282e-10\r",
      "Epoch: 2, loss: 1.1632485982953641e-10\r",
      "Epoch: 2, loss: 7.360978049399273e-10\r",
      "Epoch: 2, loss: 2.6693927446409552e-11\r",
      "Epoch: 2, loss: 3.6290754831646053e-10\r",
      "Epoch: 2, loss: 9.184383955806482e-11\r",
      "Epoch: 2, loss: 3.839029015305794e-10\r",
      "Epoch: 2, loss: 1.8908253284647863e-11\r",
      "Epoch: 2, loss: 2.2278196071256318e-10\r",
      "Epoch: 2, loss: 1.0906985165634551e-10\r",
      "Epoch: 2, loss: 1.5072229060587735e-09\r",
      "Epoch: 2, loss: 1.7938148343152648e-09\r",
      "Epoch: 2, loss: 1.1274032194266136e-10\r",
      "Epoch: 2, loss: 4.81607244287048e-10\r",
      "Epoch: 2, loss: 1.2931694617631162e-10\r",
      "Epoch: 2, loss: 9.218147300517217e-10\r",
      "Epoch: 2, loss: 4.3563638785061496e-11\r",
      "Epoch: 2, loss: 4.905372765965538e-10\r",
      "Epoch: 2, loss: 2.876898786477582e-10\r",
      "Epoch: 2, loss: 8.11689945266925e-10\r",
      "Epoch: 2, loss: 1.967951038332471e-10\r",
      "Epoch: 2, loss: 5.4003995978303234e-11\r",
      "Epoch: 2, loss: 4.9206678226559146e-11\r",
      "Epoch: 2, loss: 2.0229211260314886e-10\r",
      "Epoch: 2, loss: 2.982356019802509e-10\r",
      "Epoch: 2, loss: 1.3033468388573944e-10\r",
      "Epoch: 2, loss: 5.967900637687606e-10\r",
      "Epoch: 2, loss: 1.0002568937283367e-10\r",
      "Epoch: 2, loss: 1.9068118947783395e-11\r",
      "Epoch: 2, loss: 6.894361784433806e-10\r",
      "Epoch: 2, loss: 1.77298786336992e-10\r",
      "Epoch: 2, loss: 3.01870748492018e-11\r",
      "Epoch: 2, loss: 6.921155620753718e-10\r",
      "Epoch: 2, loss: 8.63301434716069e-10\r",
      "Epoch: 2, loss: 3.8871097885466765e-10\r",
      "Epoch: 2, loss: 2.94608513551644e-11\r",
      "Epoch: 2, loss: 1.7323240248196558e-11\r",
      "Epoch: 2, loss: 3.0308859981798257e-11\r",
      "Epoch: 2, loss: 2.942502891444989e-11\r",
      "Epoch: 2, loss: 1.6590619471612018e-10\r",
      "Epoch: 2, loss: 6.829300591767239e-11\r",
      "Epoch: 2, loss: 5.648074535950033e-10\r",
      "Epoch: 2, loss: 5.207481121771766e-12\r",
      "Epoch: 2, loss: 6.841910586844349e-10\r",
      "Epoch: 2, loss: 5.826302580484097e-11\r",
      "Epoch: 2, loss: 4.656514544157976e-10\r",
      "Epoch: 2, loss: 2.434058060002432e-10\r",
      "Epoch: 2, loss: 1.623715689407674e-10\r",
      "Epoch: 2, loss: 6.245188082156866e-10\r",
      "Epoch: 2, loss: 7.63097509017382e-10\r",
      "Epoch: 2, loss: 1.8667701194131358e-10\r",
      "Epoch: 2, loss: 4.834105874221875e-10\r",
      "Epoch: 2, loss: 4.692922204957354e-10\r",
      "Epoch: 2, loss: 7.108792490283064e-10\r",
      "Epoch: 2, loss: 7.16948679181057e-12\r",
      "Epoch: 2, loss: 6.684454082321226e-11\r",
      "Epoch: 2, loss: 3.166530504508437e-10\r",
      "Epoch: 2, loss: 2.1126259103615535e-10\r",
      "Epoch: 2, loss: 5.561364798864191e-12\r",
      "Epoch: 2, loss: 8.46766480894887e-11\r",
      "Epoch: 2, loss: 2.1125599758078443e-10\r",
      "Epoch: 2, loss: 4.245940872323632e-12\r",
      "Epoch: 2, loss: 1.316346020435207e-10\r",
      "Epoch: 2, loss: 1.9074338191517243e-10\r",
      "Epoch: 2, loss: 7.058516614979375e-11\r",
      "Epoch: 2, loss: 5.993715214715442e-11\r",
      "Epoch: 2, loss: 4.338489709697793e-10\r",
      "Epoch: 2, loss: 2.6226757089770094e-11\r",
      "Epoch: 3, loss: 3.422135612190185e-10\r",
      "Epoch: 3, loss: 7.426627380427349e-11\r",
      "Epoch: 3, loss: 3.1370348758640443e-10\r",
      "Epoch: 3, loss: 3.7900502312394913e-10\r",
      "Epoch: 3, loss: 1.9425344642751857e-10\r",
      "Epoch: 3, loss: 6.023780037563166e-11\r",
      "Epoch: 3, loss: 8.316346803576376e-11\r",
      "Epoch: 3, loss: 1.7229204116882804e-10\r",
      "Epoch: 3, loss: 6.477197572150919e-11\r",
      "Epoch: 3, loss: 1.693224175893525e-10\r",
      "Epoch: 3, loss: 1.0167718990126782e-11\r",
      "Epoch: 3, loss: 6.289176967106147e-12\r",
      "Epoch: 3, loss: 2.1438927338085318e-11\r",
      "Epoch: 3, loss: 1.2756838031199852e-10\r",
      "Epoch: 3, loss: 1.7433389570034166e-10\r",
      "Epoch: 3, loss: 5.261395238784697e-11\r",
      "Epoch: 3, loss: 2.5106395000449e-10\r",
      "Epoch: 3, loss: 1.5000626437162105e-10\r",
      "Epoch: 3, loss: 1.842190279752254e-10\r",
      "Epoch: 3, loss: 1.0547765359990566e-10\r",
      "Epoch: 3, loss: 4.694724467903841e-11\r",
      "Epoch: 3, loss: 1.2799818344212694e-10\r",
      "Epoch: 3, loss: 5.267053253485848e-11\r",
      "Epoch: 3, loss: 1.4859140752359136e-10\r",
      "Epoch: 3, loss: 7.203048197423705e-11\r",
      "Epoch: 3, loss: 8.645073124463649e-11\r",
      "Epoch: 3, loss: 6.755319216924887e-12\r",
      "Epoch: 3, loss: 4.1348167970185736e-11\r",
      "Epoch: 3, loss: 3.4038237576876874e-11\r",
      "Epoch: 3, loss: 1.8520746855830054e-11\r",
      "Epoch: 3, loss: 2.6647471715974105e-10\r",
      "Epoch: 3, loss: 4.849469566711893e-11\r",
      "Epoch: 3, loss: 1.0325838504108702e-10\r",
      "Epoch: 3, loss: 1.7941169552704606e-11\r",
      "Epoch: 3, loss: 1.0250792358078207e-10\r",
      "Epoch: 3, loss: 4.505554837086151e-12\r",
      "Epoch: 3, loss: 5.1163216303198946e-11\r",
      "Epoch: 3, loss: 1.4208678690199566e-11\r",
      "Epoch: 3, loss: 5.391974332033652e-11\r",
      "Epoch: 3, loss: 2.0177744695732916e-12\r",
      "Epoch: 3, loss: 3.235562578517956e-11\r",
      "Epoch: 3, loss: 1.6682910415787282e-11\r",
      "Epoch: 3, loss: 1.9954543563708704e-10\r",
      "Epoch: 3, loss: 2.3737521007604987e-10\r",
      "Epoch: 3, loss: 1.6827526607104217e-11\r",
      "Epoch: 3, loss: 6.565196473387196e-11\r",
      "Epoch: 3, loss: 1.9094912551260555e-11\r",
      "Epoch: 3, loss: 1.2406285710395047e-10\r",
      "Epoch: 3, loss: 6.822611352178202e-12\r",
      "Epoch: 3, loss: 6.768674874573723e-11\r",
      "Epoch: 3, loss: 4.064776955731804e-11\r",
      "Epoch: 3, loss: 1.097562787644585e-10\r",
      "Epoch: 3, loss: 2.8439715034864724e-11\r",
      "Epoch: 3, loss: 8.525064400675011e-12\r",
      "Epoch: 3, loss: 7.865975460736975e-12\r",
      "Epoch: 3, loss: 2.8842046535317616e-11\r",
      "Epoch: 3, loss: 4.195022336511784e-11\r",
      "Epoch: 3, loss: 1.8916285740110197e-11\r",
      "Epoch: 3, loss: 8.055263408461317e-11\r",
      "Epoch: 3, loss: 1.4920278053237154e-11\r",
      "Epoch: 3, loss: 1.6829825383328762e-12\r",
      "Epoch: 3, loss: 9.26637633471408e-11\r",
      "Epoch: 3, loss: 2.5358680160478568e-11\r",
      "Epoch: 3, loss: 4.918564274396038e-12\r",
      "Epoch: 3, loss: 9.21253369521229e-11\r",
      "Epoch: 3, loss: 1.1462444741477831e-10\r",
      "Epoch: 3, loss: 5.3083383585477785e-11\r",
      "Epoch: 3, loss: 4.852523872416926e-12\r",
      "Epoch: 3, loss: 1.5371854211992931e-12\r",
      "Epoch: 3, loss: 4.649938196901841e-12\r",
      "Epoch: 3, loss: 2.6769319849730197e-12\r",
      "Epoch: 3, loss: 1.8359710176539073e-11\r",
      "Epoch: 3, loss: 7.205101945546768e-12\r",
      "Epoch: 3, loss: 7.675953747276709e-11\r",
      "Epoch: 3, loss: 7.567457540856902e-13\r",
      "Epoch: 3, loss: 9.235520780863755e-11\r",
      "Epoch: 3, loss: 8.707765756369872e-12\r",
      "Epoch: 3, loss: 6.269544053679938e-11\r",
      "Epoch: 3, loss: 3.3618739359657754e-11\r",
      "Epoch: 3, loss: 2.287362465914842e-11\r",
      "Epoch: 3, loss: 8.381353479910233e-11\r",
      "Epoch: 3, loss: 1.0109966610876805e-10\r",
      "Epoch: 3, loss: 2.5944753817084828e-11\r",
      "Epoch: 3, loss: 6.533566685751265e-11\r",
      "Epoch: 3, loss: 6.274096018513995e-11\r",
      "Epoch: 3, loss: 9.384506528804053e-11\r",
      "Epoch: 3, loss: 6.468945545988947e-13\r",
      "Epoch: 3, loss: 9.649341505465571e-12\r",
      "Epoch: 3, loss: 4.334024387142127e-11\r",
      "Epoch: 3, loss: 2.8743906854191677e-11\r",
      "Epoch: 3, loss: 6.599716352370441e-13\r",
      "Epoch: 3, loss: 1.2172362356458906e-11\r",
      "Epoch: 3, loss: 2.898663631725129e-11\r",
      "Epoch: 3, loss: 4.859394948900459e-13\r",
      "Epoch: 3, loss: 1.8453832450358892e-11\r",
      "Epoch: 3, loss: 2.6481661234270136e-11\r",
      "Epoch: 3, loss: 1.0193232902605326e-11\r",
      "Epoch: 3, loss: 8.741569303604748e-12\r",
      "Epoch: 3, loss: 5.82615108559585e-11\r",
      "Epoch: 3, loss: 4.016737504689954e-12\r",
      "Epoch: 4, loss: 4.576579450353791e-11\r",
      "Epoch: 4, loss: 1.0637205605167623e-11\r",
      "Epoch: 4, loss: 4.233555424358931e-11\r",
      "Epoch: 4, loss: 5.073174587762273e-11\r",
      "Epoch: 4, loss: 2.6483949373609957e-11\r",
      "Epoch: 4, loss: 8.529935012523981e-12\r",
      "Epoch: 4, loss: 1.160192279385478e-11\r",
      "Epoch: 4, loss: 2.340769822122256e-11\r",
      "Epoch: 4, loss: 9.253757978599065e-12\r",
      "Epoch: 4, loss: 2.2964733737290226e-11\r",
      "Epoch: 4, loss: 1.678818959003891e-12\r",
      "Epoch: 4, loss: 1.0330094208575541e-12\r",
      "Epoch: 4, loss: 2.1990432164809304e-12\r",
      "Epoch: 4, loss: 1.7781870809718293e-11\r",
      "Epoch: 4, loss: 2.3605461586876502e-11\r",
      "Epoch: 4, loss: 7.299749123894426e-12\r",
      "Epoch: 4, loss: 3.3797407886912686e-11\r",
      "Epoch: 4, loss: 2.0392990651884605e-11\r",
      "Epoch: 4, loss: 2.501032264732386e-11\r",
      "Epoch: 4, loss: 1.4477197838785018e-11\r",
      "Epoch: 4, loss: 6.808592308273338e-12\r",
      "Epoch: 4, loss: 1.7483747903582964e-11\r",
      "Epoch: 4, loss: 7.365172154383108e-12\r",
      "Epoch: 4, loss: 2.0743660549521307e-11\r",
      "Epoch: 4, loss: 1.0062453753173118e-11\r",
      "Epoch: 4, loss: 1.2200945505194433e-11\r",
      "Epoch: 4, loss: 1.1132251410001834e-12\r",
      "Epoch: 4, loss: 5.8919268777811865e-12\r",
      "Epoch: 4, loss: 4.990733325797511e-12\r",
      "Epoch: 4, loss: 2.8056158419359846e-12\r",
      "Epoch: 4, loss: 3.46942579551367e-11\r",
      "Epoch: 4, loss: 6.695290761473043e-12\r",
      "Epoch: 4, loss: 1.4262658333808068e-11\r",
      "Epoch: 4, loss: 2.721307593149041e-12\r",
      "Epoch: 4, loss: 1.4230891124671169e-11\r",
      "Epoch: 4, loss: 7.761103151043935e-13\r",
      "Epoch: 4, loss: 7.1864215421532965e-12\r",
      "Epoch: 4, loss: 2.162444940552124e-12\r",
      "Epoch: 4, loss: 7.547923737142827e-12\r",
      "Epoch: 4, loss: 2.6507771441959066e-13\r",
      "Epoch: 4, loss: 4.6599869989695055e-12\r",
      "Epoch: 4, loss: 2.5092705489306176e-12\r",
      "Epoch: 4, loss: 2.6506733903663033e-11\r",
      "Epoch: 4, loss: 3.149815778021416e-11\r",
      "Epoch: 4, loss: 2.486638446747322e-12\r",
      "Epoch: 4, loss: 8.959507246419359e-12\r",
      "Epoch: 4, loss: 2.793460777016414e-12\r",
      "Epoch: 4, loss: 1.6717806299558255e-11\r",
      "Epoch: 4, loss: 1.0556635548005744e-12\r",
      "Epoch: 4, loss: 9.32589465718731e-12\r",
      "Epoch: 4, loss: 5.7184245684834194e-12\r",
      "Epoch: 4, loss: 1.4849736226913108e-11\r",
      "Epoch: 4, loss: 4.0785751144167986e-12\r",
      "Epoch: 4, loss: 1.320176212455001e-12\r",
      "Epoch: 4, loss: 1.2298202751525913e-12\r",
      "Epoch: 4, loss: 4.09104159656242e-12\r",
      "Epoch: 4, loss: 5.875264337953225e-12\r",
      "Epoch: 4, loss: 2.7281865447798642e-12\r",
      "Epoch: 4, loss: 1.0887082217471938e-11\r",
      "Epoch: 4, loss: 2.1989634305515234e-12\r",
      "Epoch: 4, loss: 1.495702118709206e-13\r",
      "Epoch: 4, loss: 1.2473735149844601e-11\r",
      "Epoch: 4, loss: 3.605364821730095e-12\r",
      "Epoch: 4, loss: 7.830022557461439e-13\r",
      "Epoch: 4, loss: 1.2301553357031845e-11\r",
      "Epoch: 4, loss: 1.5261665609323753e-11\r",
      "Epoch: 4, loss: 7.2489106094144515e-12\r",
      "Epoch: 4, loss: 7.785569483627334e-13\r",
      "Epoch: 4, loss: 1.355238711375425e-13\r",
      "Epoch: 4, loss: 7.072021094291611e-13\r",
      "Epoch: 4, loss: 2.381316442395729e-13\r",
      "Epoch: 4, loss: 2.0357517849999115e-12\r",
      "Epoch: 4, loss: 7.568378417891646e-13\r",
      "Epoch: 4, loss: 1.0431824565252385e-11\r",
      "Epoch: 4, loss: 1.2124630418194048e-13\r",
      "Epoch: 4, loss: 1.2475760038638258e-11\r",
      "Epoch: 4, loss: 1.2862002342093877e-12\r",
      "Epoch: 4, loss: 8.456001671890915e-12\r",
      "Epoch: 4, loss: 4.637532173940792e-12\r",
      "Epoch: 4, loss: 3.209830357171834e-12\r",
      "Epoch: 4, loss: 1.1265459366383514e-11\r",
      "Epoch: 4, loss: 1.3436144235162612e-11\r",
      "Epoch: 4, loss: 3.5990231098737067e-12\r",
      "Epoch: 4, loss: 8.835949803207616e-12\r",
      "Epoch: 4, loss: 8.407680988332384e-12\r",
      "Epoch: 4, loss: 1.2430213056312226e-11\r",
      "Epoch: 4, loss: 6.369402447259517e-14\r",
      "Epoch: 4, loss: 1.3849787336977081e-12\r",
      "Epoch: 4, loss: 5.927598840651085e-12\r",
      "Epoch: 4, loss: 3.9144915382598505e-12\r",
      "Epoch: 4, loss: 9.521017483137663e-14\r",
      "Epoch: 4, loss: 1.7387407009526497e-12\r",
      "Epoch: 4, loss: 3.9751729314322235e-12\r",
      "Epoch: 4, loss: 6.754762593659584e-14\r",
      "Epoch: 4, loss: 2.5788129859802666e-12\r",
      "Epoch: 4, loss: 3.667559925192322e-12\r",
      "Epoch: 4, loss: 1.4617138981170476e-12\r",
      "Epoch: 4, loss: 1.2640786214109126e-12\r",
      "Epoch: 4, loss: 7.834317412028062e-12\r",
      "Epoch: 4, loss: 6.062704822826856e-13\r",
      "Epoch: 5, loss: 6.1350531967939675e-12\r",
      "Epoch: 5, loss: 1.5145684370369907e-12\r",
      "Epoch: 5, loss: 5.7185082721285086e-12\r",
      "Epoch: 5, loss: 6.8023820287730296e-12\r",
      "Epoch: 5, loss: 3.610897590917157e-12\r",
      "Epoch: 5, loss: 1.2057976167581073e-12\r",
      "Epoch: 5, loss: 1.6162191813337689e-12\r",
      "Epoch: 5, loss: 3.182661800726759e-12\r",
      "Epoch: 5, loss: 1.3150034314612717e-12\r",
      "Epoch: 5, loss: 3.117641369040576e-12\r",
      "Epoch: 5, loss: 2.7087502992943055e-13\r",
      "Epoch: 5, loss: 1.6771310123846984e-13\r",
      "Epoch: 5, loss: 2.2348032976920687e-13\r",
      "Epoch: 5, loss: 2.4716360443626513e-12\r",
      "Epoch: 5, loss: 3.1996041213915528e-12\r",
      "Epoch: 5, loss: 1.0125346658403772e-12\r",
      "Epoch: 5, loss: 4.555819647535533e-12\r",
      "Epoch: 5, loss: 2.77391011879697e-12\r",
      "Epoch: 5, loss: 3.397052569384548e-12\r",
      "Epoch: 5, loss: 1.986308195151018e-12\r",
      "Epoch: 5, loss: 9.796579630505803e-13\r",
      "Epoch: 5, loss: 2.3880995111129106e-12\r",
      "Epoch: 5, loss: 1.0278760505994406e-12\r",
      "Epoch: 5, loss: 2.8857608581651015e-12\r",
      "Epoch: 5, loss: 1.4020807646094423e-12\r",
      "Epoch: 5, loss: 1.7138139681966136e-12\r",
      "Epoch: 5, loss: 1.7965913606783373e-13\r",
      "Epoch: 5, loss: 8.356789322577014e-13\r",
      "Epoch: 5, loss: 7.247237374125365e-13\r",
      "Epoch: 5, loss: 4.1914001144360814e-13\r",
      "Epoch: 5, loss: 4.539576800375096e-12\r",
      "Epoch: 5, loss: 9.240810979713608e-13\r",
      "Epoch: 5, loss: 1.9668902400712312e-12\r",
      "Epoch: 5, loss: 4.0691849047850465e-13\r",
      "Epoch: 5, loss: 1.9708535300963275e-12\r",
      "Epoch: 5, loss: 1.3165484718869709e-13\r",
      "Epoch: 5, loss: 1.006118130643561e-12\r",
      "Epoch: 5, loss: 3.244131900037421e-13\r",
      "Epoch: 5, loss: 1.0535223427601438e-12\r",
      "Epoch: 5, loss: 4.1215562415288595e-14\r",
      "Epoch: 5, loss: 6.66475978280711e-13\r",
      "Epoch: 5, loss: 3.7226855397204164e-13\r",
      "Epoch: 5, loss: 3.531377538488583e-12\r",
      "Epoch: 5, loss: 4.190511724304257e-12\r",
      "Epoch: 5, loss: 3.6404851392571086e-13\r",
      "Epoch: 5, loss: 1.223505480610741e-12\r",
      "Epoch: 5, loss: 4.0527176586565585e-13\r",
      "Epoch: 5, loss: 2.2554911416308285e-12\r",
      "Epoch: 5, loss: 1.611131723463981e-13\r",
      "Epoch: 5, loss: 1.2833814100566542e-12\r",
      "Epoch: 5, loss: 8.015328100551281e-13\r",
      "Epoch: 5, loss: 2.0104856281179343e-12\r",
      "Epoch: 5, loss: 5.811832385467268e-13\r",
      "Epoch: 5, loss: 2.0099808315545944e-13\r",
      "Epoch: 5, loss: 1.8864249646809014e-13\r",
      "Epoch: 5, loss: 5.776690424589841e-13\r",
      "Epoch: 5, loss: 8.199892360790277e-13\r",
      "Epoch: 5, loss: 3.911180262707247e-13\r",
      "Epoch: 5, loss: 1.4731331895318264e-12\r",
      "Epoch: 5, loss: 3.208144452630475e-13\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, loss: 1.4133436556831851e-14\r",
      "Epoch: 5, loss: 1.6815142944903537e-12\r",
      "Epoch: 5, loss: 5.099970819617703e-13\r",
      "Epoch: 5, loss: 1.2204132154177e-13\r",
      "Epoch: 5, loss: 1.6470172498536145e-12\r",
      "Epoch: 5, loss: 2.0371526193727476e-12\r",
      "Epoch: 5, loss: 9.898648426587837e-13\r",
      "Epoch: 5, loss: 1.2205630879170996e-13\r",
      "Epoch: 5, loss: 1.234218513889835e-14\r",
      "Epoch: 5, loss: 1.0640382422486501e-13\r",
      "Epoch: 5, loss: 2.0890345907401122e-14\r",
      "Epoch: 5, loss: 2.2622848417138313e-13\r",
      "Epoch: 5, loss: 7.911122448691995e-14\r",
      "Epoch: 5, loss: 1.4179569019304737e-12\r",
      "Epoch: 5, loss: 2.007212531741561e-14\r",
      "Epoch: 5, loss: 1.686675260026062e-12\r",
      "Epoch: 5, loss: 1.8806255184260244e-13\r",
      "Epoch: 5, loss: 1.1421705360633513e-12\r",
      "Epoch: 5, loss: 6.390077059841259e-13\r",
      "Epoch: 5, loss: 4.489424441098996e-13\r",
      "Epoch: 5, loss: 1.5164242040042298e-12\r",
      "Epoch: 5, loss: 1.79064459539041e-12\r",
      "Epoch: 5, loss: 4.983962354452482e-13\r",
      "Epoch: 5, loss: 1.1958218755659672e-12\r",
      "Epoch: 5, loss: 1.1289923131028884e-12\r",
      "Epoch: 5, loss: 1.6514260134555322e-12\r",
      "Epoch: 5, loss: 7.418910693153522e-15\r",
      "Epoch: 5, loss: 1.9771041193088e-13\r",
      "Epoch: 5, loss: 8.103178034807361e-13\r",
      "Epoch: 5, loss: 5.334370796859964e-13\r",
      "Epoch: 5, loss: 1.5485459164902867e-14\r",
      "Epoch: 5, loss: 2.470060431797145e-13\r",
      "Epoch: 5, loss: 5.448905469505606e-13\r",
      "Epoch: 5, loss: 1.0739369283498131e-14\r",
      "Epoch: 5, loss: 3.593834691646882e-13\r",
      "Epoch: 5, loss: 5.069432472015319e-13\r",
      "Epoch: 5, loss: 2.0834473848765235e-13\r",
      "Epoch: 5, loss: 1.8146466592633165e-13\r",
      "Epoch: 5, loss: 1.0548678460396183e-12\r",
      "Epoch: 5, loss: 9.033209704545843e-14\r",
      "Epoch: 6, loss: 8.241187569013579e-13\r",
      "Epoch: 6, loss: 2.1454933133949323e-13\r",
      "Epoch: 6, loss: 7.731456310887039e-13\r",
      "Epoch: 6, loss: 9.136130695034888e-13\r",
      "Epoch: 6, loss: 4.923607249983409e-13\r",
      "Epoch: 6, loss: 1.7002270953251491e-13\r",
      "Epoch: 6, loss: 2.2477733179910212e-13\r",
      "Epoch: 6, loss: 4.3300030498272886e-13\r",
      "Epoch: 6, loss: 1.8598981402744695e-13\r",
      "Epoch: 6, loss: 4.2357266971118196e-13\r",
      "Epoch: 6, loss: 4.27375283440473e-14\r",
      "Epoch: 6, loss: 2.6732646254583012e-14\r",
      "Epoch: 6, loss: 2.247145302874827e-14\r",
      "Epoch: 6, loss: 3.42755288669339e-13\r",
      "Epoch: 6, loss: 4.340730713159142e-13\r",
      "Epoch: 6, loss: 1.4033494871795525e-13\r",
      "Epoch: 6, loss: 6.148869952310279e-13\r",
      "Epoch: 6, loss: 3.774906332742112e-13\r",
      "Epoch: 6, loss: 4.616222900647677e-13\r",
      "Epoch: 6, loss: 2.7241731920692463e-13\r",
      "Epoch: 6, loss: 1.40018406302847e-13\r",
      "Epoch: 6, loss: 3.2618055414987344e-13\r",
      "Epoch: 6, loss: 1.4315888665149173e-13\r",
      "Epoch: 6, loss: 4.0035973870302714e-13\r",
      "Epoch: 6, loss: 1.9491681757403528e-13\r",
      "Epoch: 6, loss: 2.398114090547882e-13\r",
      "Epoch: 6, loss: 2.8377134615443612e-14\r",
      "Epoch: 6, loss: 1.1802599432109017e-13\r",
      "Epoch: 6, loss: 1.0439202805008281e-13\r",
      "Epoch: 6, loss: 6.187528653291707e-14\r",
      "Epoch: 6, loss: 5.965858746511226e-13\r",
      "Epoch: 6, loss: 1.274631034278274e-13\r",
      "Epoch: 6, loss: 2.708903551167137e-13\r",
      "Epoch: 6, loss: 6.011212699488857e-14\r",
      "Epoch: 6, loss: 2.724165738559826e-13\r",
      "Epoch: 6, loss: 2.1766143556222226e-14\r",
      "Epoch: 6, loss: 1.404541899603852e-13\r",
      "Epoch: 6, loss: 4.80687464036921e-14\r",
      "Epoch: 6, loss: 1.4667429222706015e-13\r",
      "Epoch: 6, loss: 6.920659456162676e-15\r",
      "Epoch: 6, loss: 9.475889219848614e-14\r",
      "Epoch: 6, loss: 5.4603340268920525e-14\r",
      "Epoch: 6, loss: 4.71692220552713e-13\r",
      "Epoch: 6, loss: 5.588688533933074e-13\r",
      "Epoch: 6, loss: 5.2851980849324746e-14\r",
      "Epoch: 6, loss: 1.6714365547887236e-13\r",
      "Epoch: 6, loss: 5.836460523861841e-14\r",
      "Epoch: 6, loss: 3.0464930165827763e-13\r",
      "Epoch: 6, loss: 2.4262357340698297e-14\r",
      "Epoch: 6, loss: 1.7643724112510548e-13\r",
      "Epoch: 6, loss: 1.1199433084024625e-13\r",
      "Epoch: 6, loss: 2.723897567337953e-13\r",
      "Epoch: 6, loss: 8.237001706200674e-14\r",
      "Epoch: 6, loss: 3.015646530951084e-14\r",
      "Epoch: 6, loss: 2.847116710076497e-14\r",
      "Epoch: 6, loss: 8.124661148574655e-14\r",
      "Epoch: 6, loss: 1.141160594851354e-13\r",
      "Epoch: 6, loss: 5.5763364558230144e-14\r",
      "Epoch: 6, loss: 1.9953061160490425e-13\r",
      "Epoch: 6, loss: 4.6403740338799393e-14\r",
      "Epoch: 6, loss: 1.5430812138115171e-15\r",
      "Epoch: 6, loss: 2.2697037753403583e-13\r",
      "Epoch: 6, loss: 7.182904438854565e-14\r",
      "Epoch: 6, loss: 1.8675538270739667e-14\r",
      "Epoch: 6, loss: 2.2101521382744655e-13\r",
      "Epoch: 6, loss: 2.7254552608216714e-13\r",
      "Epoch: 6, loss: 1.3516750868425789e-13\r",
      "Epoch: 6, loss: 1.8761128181800164e-14\r",
      "Epoch: 6, loss: 1.2501577352002072e-15\r",
      "Epoch: 6, loss: 1.583540161060839e-14\r",
      "Epoch: 6, loss: 1.8581248523945268e-15\r",
      "Epoch: 6, loss: 2.5202487664913858e-14\r",
      "Epoch: 6, loss: 8.223595284637762e-15\r",
      "Epoch: 6, loss: 1.927897035091455e-13\r",
      "Epoch: 6, loss: 3.3131542740327266e-15\r",
      "Epoch: 6, loss: 2.28225475840873e-13\r",
      "Epoch: 6, loss: 2.7258539376714763e-14\r",
      "Epoch: 6, loss: 1.5446928113859922e-13\r",
      "Epoch: 6, loss: 8.79613739332729e-14\r",
      "Epoch: 6, loss: 6.26119221674769e-14\r",
      "Epoch: 6, loss: 2.044045683986669e-13\r",
      "Epoch: 6, loss: 2.3923673029394893e-13\r",
      "Epoch: 6, loss: 6.891212885099237e-14\r",
      "Epoch: 6, loss: 1.6195843543094096e-13\r",
      "Epoch: 6, loss: 1.5187613025753222e-13\r",
      "Epoch: 6, loss: 2.2000312206666746e-13\r",
      "Epoch: 6, loss: 1.0446496061021389e-15\r",
      "Epoch: 6, loss: 2.8083370205563174e-14\r",
      "Epoch: 6, loss: 1.1073683166338046e-13\r",
      "Epoch: 6, loss: 7.2725217087062e-14\r",
      "Epoch: 6, loss: 2.6193763516883925e-15\r",
      "Epoch: 6, loss: 3.492255733021777e-14\r",
      "Epoch: 6, loss: 7.465867726368933e-14\r",
      "Epoch: 6, loss: 1.7997841525700835e-15\r",
      "Epoch: 6, loss: 4.9964102221728355e-14\r",
      "Epoch: 6, loss: 6.995956457490326e-14\r",
      "Epoch: 6, loss: 2.9541128181501015e-14\r",
      "Epoch: 6, loss: 2.5887694146654287e-14\r",
      "Epoch: 6, loss: 1.4221765340218485e-13\r",
      "Epoch: 6, loss: 1.3308334959940516e-14\r",
      "Epoch: 7, loss: 1.1090273608407028e-13\r",
      "Epoch: 7, loss: 3.0257936013642114e-14\r",
      "Epoch: 7, loss: 1.0462424233271037e-13\r",
      "Epoch: 7, loss: 1.2289775294273459e-13\r",
      "Epoch: 7, loss: 6.714228550925551e-14\r",
      "Epoch: 7, loss: 2.3906441476149023e-14\r",
      "Epoch: 7, loss: 3.120824176553568e-14\r",
      "Epoch: 7, loss: 5.893862414891651e-14\r",
      "Epoch: 7, loss: 2.6197075481452515e-14\r",
      "Epoch: 7, loss: 5.75847328649025e-14\r",
      "Epoch: 7, loss: 6.609692684352639e-15\r",
      "Epoch: 7, loss: 4.179946782194293e-15\r",
      "Epoch: 7, loss: 2.2319082569283002e-15\r",
      "Epoch: 7, loss: 4.743970145012975e-14\r",
      "Epoch: 7, loss: 5.89328344689992e-14\r",
      "Epoch: 7, loss: 1.942936068688999e-14\r",
      "Epoch: 7, loss: 8.308602653586667e-14\r",
      "Epoch: 7, loss: 5.1391694724458446e-14\r",
      "Epoch: 7, loss: 6.275774293591838e-14\r",
      "Epoch: 7, loss: 3.734640793315799e-14\r",
      "Epoch: 7, loss: 1.9898208905313713e-14\r",
      "Epoch: 7, loss: 4.455047066995945e-14\r",
      "Epoch: 7, loss: 1.9899536948222288e-14\r",
      "Epoch: 7, loss: 5.542313682729763e-14\r",
      "Epoch: 7, loss: 2.7042435535294026e-14\r",
      "Epoch: 7, loss: 3.3450359280066703e-14\r",
      "Epoch: 7, loss: 4.3947943639209035e-15\r",
      "Epoch: 7, loss: 1.6605665129421128e-14\r",
      "Epoch: 7, loss: 1.493438057020734e-14\r",
      "Epoch: 7, loss: 9.04174261577739e-15\r",
      "Epoch: 7, loss: 7.870798216509561e-14\r",
      "Epoch: 7, loss: 1.7568328754331863e-14\r",
      "Epoch: 7, loss: 3.7268068854414946e-14\r",
      "Epoch: 7, loss: 8.788706007075791e-15\r",
      "Epoch: 7, loss: 3.759432475287806e-14\r",
      "Epoch: 7, loss: 3.50558258972329e-15\r",
      "Epoch: 7, loss: 1.955772947972023e-14\r",
      "Epoch: 7, loss: 7.046899626162939e-15\r",
      "Epoch: 7, loss: 2.037483290359642e-14\r",
      "Epoch: 7, loss: 1.1763619657953036e-15\r",
      "Epoch: 7, loss: 1.3405062682634125e-14\r",
      "Epoch: 7, loss: 7.933021794755045e-15\r",
      "Epoch: 7, loss: 6.315027641856829e-14\r",
      "Epoch: 7, loss: 7.47028652302314e-14\r",
      "Epoch: 7, loss: 7.616121333420903e-15\r",
      "Epoch: 7, loss: 2.2838064643178133e-14\r",
      "Epoch: 7, loss: 8.35112344612747e-15\r",
      "Epoch: 7, loss: 4.119296269533321e-14\r",
      "Epoch: 7, loss: 3.609425482310068e-15\r",
      "Epoch: 7, loss: 2.423611275144266e-14\r",
      "Epoch: 7, loss: 1.560578249820244e-14\r",
      "Epoch: 7, loss: 3.693054387915818e-14\r",
      "Epoch: 7, loss: 1.162040541269804e-14\r",
      "Epoch: 7, loss: 4.468005454996334e-15\r",
      "Epoch: 7, loss: 4.2386489879316246e-15\r",
      "Epoch: 7, loss: 1.1387420434713311e-14\r",
      "Epoch: 7, loss: 1.5843350667318658e-14\r",
      "Epoch: 7, loss: 7.911045420349676e-15\r",
      "Epoch: 7, loss: 2.7049681065566287e-14\r",
      "Epoch: 7, loss: 6.6629202361631144e-15\r",
      "Epoch: 7, loss: 2.048353643341832e-16\r",
      "Epoch: 7, loss: 3.0672656669908025e-14\r",
      "Epoch: 7, loss: 1.007876774270255e-14\r",
      "Epoch: 7, loss: 2.813375030000256e-15\r",
      "Epoch: 7, loss: 2.9716439672496906e-14\r",
      "Epoch: 7, loss: 3.65387905671645e-14\r",
      "Epoch: 7, loss: 1.8457151278958306e-14\r",
      "Epoch: 7, loss: 2.8361565878396276e-15\r",
      "Epoch: 7, loss: 1.5203230658720185e-16\r",
      "Epoch: 7, loss: 2.3327192954883036e-15\r",
      "Epoch: 7, loss: 1.7831797872041178e-16\r",
      "Epoch: 7, loss: 2.815315237109241e-15\r",
      "Epoch: 7, loss: 8.494229125022824e-16\r",
      "Epoch: 7, loss: 2.6220651245756457e-14\r",
      "Epoch: 7, loss: 5.376696088235325e-16\r",
      "Epoch: 7, loss: 3.090703662008969e-14\r",
      "Epoch: 7, loss: 3.921360861759649e-15\r",
      "Epoch: 7, loss: 2.0913436880507656e-14\r",
      "Epoch: 7, loss: 1.2097409689527344e-14\r",
      "Epoch: 7, loss: 8.710530728274909e-15\r",
      "Epoch: 7, loss: 2.7587659033643953e-14\r",
      "Epoch: 7, loss: 3.203459732383828e-14\r",
      "Epoch: 7, loss: 9.515203268714437e-15\r",
      "Epoch: 7, loss: 2.195132756692363e-14\r",
      "Epoch: 7, loss: 2.0463607836417924e-14\r",
      "Epoch: 7, loss: 2.9381586059607487e-14\r",
      "Epoch: 7, loss: 1.6683729049065715e-16\r",
      "Epoch: 7, loss: 3.9711338095171355e-15\r",
      "Epoch: 7, loss: 1.5129868581935507e-14\r",
      "Epoch: 7, loss: 9.91801858452962e-15\r",
      "Epoch: 7, loss: 4.4110801800320426e-16\r",
      "Epoch: 7, loss: 4.916984270224956e-15\r",
      "Epoch: 7, loss: 1.0225610613325769e-14\r",
      "Epoch: 7, loss: 3.0248835592152563e-16\r",
      "Epoch: 7, loss: 6.93189597015261e-15\r",
      "Epoch: 7, loss: 9.641768964717605e-15\r",
      "Epoch: 7, loss: 4.169630570827274e-15\r",
      "Epoch: 7, loss: 3.673280497485415e-15\r",
      "Epoch: 7, loss: 1.919716309585186e-14\r",
      "Epoch: 7, loss: 1.9416614078135496e-15\r",
      "Epoch: 8, loss: 1.494795787174549e-14\r",
      "Epoch: 8, loss: 4.25086625805207e-15\r",
      "Epoch: 8, loss: 1.4170226338203841e-14\r",
      "Epoch: 8, loss: 1.6556145897371677e-14\r",
      "Epoch: 8, loss: 9.157072169432838e-15\r",
      "Epoch: 8, loss: 3.3518919224342123e-15\r",
      "Epoch: 8, loss: 4.32587095165587e-15\r",
      "Epoch: 8, loss: 8.025790674715252e-15\r",
      "Epoch: 8, loss: 3.676523519817672e-15\r",
      "Epoch: 8, loss: 7.832847203120065e-15\r",
      "Epoch: 8, loss: 1.004790376078397e-15\r",
      "Epoch: 8, loss: 6.420544804399154e-16\r",
      "Epoch: 8, loss: 2.185156053987924e-16\r",
      "Epoch: 8, loss: 6.555201098372214e-15\r",
      "Epoch: 8, loss: 8.006369942160894e-15\r",
      "Epoch: 8, loss: 2.6868490204644253e-15\r",
      "Epoch: 8, loss: 1.1238858278531147e-14\r",
      "Epoch: 8, loss: 6.998890004852245e-15\r",
      "Epoch: 8, loss: 8.53558790763762e-15\r",
      "Epoch: 8, loss: 5.117949395004658e-15\r",
      "Epoch: 8, loss: 2.8139113079985342e-15\r",
      "Epoch: 8, loss: 6.084655957559111e-15\r",
      "Epoch: 8, loss: 2.7610083310804663e-15\r",
      "Epoch: 8, loss: 7.658685541427143e-15\r",
      "Epoch: 8, loss: 3.745099784298466e-15\r",
      "Epoch: 8, loss: 4.6534655960331114e-15\r",
      "Epoch: 8, loss: 6.690165251442647e-16\r",
      "Epoch: 8, loss: 2.3283803460894494e-15\r",
      "Epoch: 8, loss: 2.124067674415158e-15\r",
      "Epoch: 8, loss: 1.3097879457957663e-15\r",
      "Epoch: 8, loss: 1.0420147809797136e-14\r",
      "Epoch: 8, loss: 2.4195071227744635e-15\r",
      "Epoch: 8, loss: 5.122520109911269e-15\r",
      "Epoch: 8, loss: 1.2736612552495519e-15\r",
      "Epoch: 8, loss: 5.181256646697296e-15\r",
      "Epoch: 8, loss: 5.514899714076899e-16\r",
      "Epoch: 8, loss: 2.7172586196871245e-15\r",
      "Epoch: 8, loss: 1.0236750899985516e-15\r",
      "Epoch: 8, loss: 2.8247594690372582e-15\r",
      "Epoch: 8, loss: 1.968321721165725e-16\r",
      "Epoch: 8, loss: 1.888163513539726e-15\r",
      "Epoch: 8, loss: 1.1432902939240857e-15\r",
      "Epoch: 8, loss: 8.472005402470106e-15\r",
      "Epoch: 8, loss: 1.0006252181863399e-14\r",
      "Epoch: 8, loss: 1.0903579813399656e-15\r",
      "Epoch: 8, loss: 3.120806837140543e-15\r",
      "Epoch: 8, loss: 1.188188896700233e-15\r",
      "Epoch: 8, loss: 5.5753678253386644e-15\r",
      "Epoch: 8, loss: 5.311874771153493e-16\r",
      "Epoch: 8, loss: 3.326806372743887e-15\r",
      "Epoch: 8, loss: 2.169423634503069e-15\r",
      "Epoch: 8, loss: 5.010410252221715e-15\r",
      "Epoch: 8, loss: 1.6328665942015565e-15\r",
      "Epoch: 8, loss: 6.549188657008985e-16\r",
      "Epoch: 8, loss: 6.237639167814803e-16\r",
      "Epoch: 8, loss: 1.5912007101475318e-15\r",
      "Epoch: 8, loss: 2.1951639007585214e-15\r",
      "Epoch: 8, loss: 1.1173691687107556e-15\r",
      "Epoch: 8, loss: 3.6699111034426e-15\r",
      "Epoch: 8, loss: 9.50700677006028e-16\r",
      "Epoch: 8, loss: 3.17344559607509e-17\r",
      "Epoch: 8, loss: 4.1495313239638446e-15\r",
      "Epoch: 8, loss: 1.4096360001767838e-15\r",
      "Epoch: 8, loss: 4.182137039532071e-16\r",
      "Epoch: 8, loss: 4.002328221988457e-15\r",
      "Epoch: 8, loss: 4.907753792246968e-15\r",
      "Epoch: 8, loss: 2.5203142652909076e-15\r",
      "Epoch: 8, loss: 4.227827471426434e-16\r",
      "Epoch: 8, loss: 2.221460238525528e-17\r",
      "Epoch: 8, loss: 3.404803573819019e-16\r",
      "Epoch: 8, loss: 2.0122452157591963e-17\r",
      "Epoch: 8, loss: 3.1543965324043275e-16\r",
      "Epoch: 8, loss: 8.709615834385494e-17\r",
      "Epoch: 8, loss: 3.5673757234434615e-15\r",
      "Epoch: 8, loss: 8.548190261409102e-17\r",
      "Epoch: 8, loss: 4.1888404714048835e-15\r",
      "Epoch: 8, loss: 5.604724985679648e-16\r",
      "Epoch: 8, loss: 2.834137903122158e-15\r",
      "Epoch: 8, loss: 1.6624551534889794e-15\r",
      "Epoch: 8, loss: 1.2091805779602526e-15\r",
      "Epoch: 8, loss: 3.727762828230335e-15\r",
      "Epoch: 8, loss: 4.298187274191131e-15\r",
      "Epoch: 8, loss: 1.3122201374344285e-15\r",
      "Epoch: 8, loss: 2.9773125112969976e-15\r",
      "Epoch: 8, loss: 2.7611626497053305e-15\r",
      "Epoch: 8, loss: 3.932747076680054e-15\r",
      "Epoch: 8, loss: 2.793494260593889e-17\r",
      "Epoch: 8, loss: 5.592877666747441e-16\r",
      "Epoch: 8, loss: 2.066877522457887e-15\r",
      "Epoch: 8, loss: 1.3528991491264136e-15\r",
      "Epoch: 8, loss: 7.27768145410693e-17\r",
      "Epoch: 8, loss: 6.897896765990098e-16\r",
      "Epoch: 8, loss: 1.4000839020278152e-15\r",
      "Epoch: 8, loss: 4.9963184068851904e-17\r",
      "Epoch: 8, loss: 9.599587024328279e-16\r",
      "Epoch: 8, loss: 1.3273257421280542e-15\r",
      "Epoch: 8, loss: 5.862072914581213e-16\r",
      "Epoch: 8, loss: 5.187888033650539e-16\r",
      "Epoch: 8, loss: 2.5942525364803387e-15\r",
      "Epoch: 8, loss: 2.809139509053726e-16\r",
      "Epoch: 9, loss: 2.0175730940583012e-15\r",
      "Epoch: 9, loss: 5.951914426044092e-16\r",
      "Epoch: 9, loss: 1.9207467628236504e-15\r",
      "Epoch: 9, loss: 2.2333600479353432e-15\r",
      "Epoch: 9, loss: 1.2490074724187886e-15\r",
      "Epoch: 9, loss: 4.686906603560324e-16\r",
      "Epoch: 9, loss: 5.986996193193939e-16\r",
      "Epoch: 9, loss: 1.0932590212178911e-15\r",
      "Epoch: 9, loss: 5.143226212050098e-16\r",
      "Epoch: 9, loss: 1.065933614657944e-15\r",
      "Epoch: 9, loss: 1.5052138962896253e-16\r",
      "Epoch: 9, loss: 9.708491645430259e-17\r",
      "Epoch: 9, loss: 2.1037558205849973e-17\r",
      "Epoch: 9, loss: 9.045199246856417e-16\r",
      "Epoch: 9, loss: 1.0883327243532975e-15\r",
      "Epoch: 9, loss: 3.7112232212418556e-16\r",
      "Epoch: 9, loss: 1.5217264170905313e-15\r",
      "Epoch: 9, loss: 9.534449845351569e-16\r",
      "Epoch: 9, loss: 1.161374112781343e-15\r",
      "Epoch: 9, loss: 7.011109894834165e-16\r",
      "Epoch: 9, loss: 3.962462593943166e-16\r",
      "Epoch: 9, loss: 8.310175286166297e-16\r",
      "Epoch: 9, loss: 3.8243430343468293e-16\r",
      "Epoch: 9, loss: 1.0567412958432241e-15\r",
      "Epoch: 9, loss: 5.17835273133508e-16\r",
      "Epoch: 9, loss: 6.459058960743476e-16\r",
      "Epoch: 9, loss: 1.0035129691647183e-16\r",
      "Epoch: 9, loss: 3.2548927168830066e-16\r",
      "Epoch: 9, loss: 3.0058640999802743e-16\r",
      "Epoch: 9, loss: 1.88322207439939e-16\r",
      "Epoch: 9, loss: 1.383833502027449e-15\r",
      "Epoch: 9, loss: 3.3294806592154186e-16\r",
      "Epoch: 9, loss: 7.035445042806714e-16\r",
      "Epoch: 9, loss: 1.8318934092928046e-16\r",
      "Epoch: 9, loss: 7.132802614037631e-16\r",
      "Epoch: 9, loss: 8.50259033778879e-17\r",
      "Epoch: 9, loss: 3.767794378555152e-16\r",
      "Epoch: 9, loss: 1.4754199327690492e-16\r",
      "Epoch: 9, loss: 3.909474227803498e-16\r",
      "Epoch: 9, loss: 3.2157818491895725e-17\r",
      "Epoch: 9, loss: 2.6496459450130253e-16\r",
      "Epoch: 9, loss: 1.6364160256799817e-16\r",
      "Epoch: 9, loss: 1.138669109627558e-15\r",
      "Epoch: 9, loss: 1.3428767423652748e-15\r",
      "Epoch: 9, loss: 1.5521070758026266e-16\r",
      "Epoch: 9, loss: 4.2646569523967694e-16\r",
      "Epoch: 9, loss: 1.6822193750541153e-16\r",
      "Epoch: 9, loss: 7.552924823837298e-16\r",
      "Epoch: 9, loss: 7.743777548094124e-17\r",
      "Epoch: 9, loss: 4.563801834667536e-16\r",
      "Epoch: 9, loss: 3.0095608987679193e-16\r",
      "Epoch: 9, loss: 6.801983184757097e-16\r",
      "Epoch: 9, loss: 2.286601592214593e-16\r",
      "Epoch: 9, loss: 9.512145479864514e-17\r",
      "Epoch: 9, loss: 9.089575747372062e-17\r",
      "Epoch: 9, loss: 2.2175100543182906e-16\r",
      "Epoch: 9, loss: 3.036212576087254e-16\r",
      "Epoch: 9, loss: 1.5720147941437698e-16\r",
      "Epoch: 9, loss: 4.982546678019841e-16\r",
      "Epoch: 9, loss: 1.349180945946473e-16\r",
      "Epoch: 9, loss: 5.2779379804586306e-18\r",
      "Epoch: 9, loss: 5.619106005715723e-16\r",
      "Epoch: 9, loss: 1.9659913924782395e-16\r",
      "Epoch: 9, loss: 6.146934937682654e-17\r",
      "Epoch: 9, loss: 5.39858151854457e-16\r",
      "Epoch: 9, loss: 6.603073634033991e-16\r",
      "Epoch: 9, loss: 3.4414657071978633e-16\r",
      "Epoch: 9, loss: 6.228316880687641e-17\r",
      "Epoch: 9, loss: 3.61540941641659e-18\r",
      "Epoch: 9, loss: 4.929196678090171e-17\r",
      "Epoch: 9, loss: 2.770698794868452e-18\r",
      "Epoch: 9, loss: 3.5459580993027737e-17\r",
      "Epoch: 9, loss: 8.854631880644586e-18\r",
      "Epoch: 9, loss: 4.855104024327296e-16\r",
      "Epoch: 9, loss: 1.3323054050967844e-17\r",
      "Epoch: 9, loss: 5.681341436230773e-16\r",
      "Epoch: 9, loss: 7.965939291470953e-17\r",
      "Epoch: 9, loss: 3.8439601746826474e-16\r",
      "Epoch: 9, loss: 2.2829831352686694e-16\r",
      "Epoch: 9, loss: 1.6753823641494335e-16\r",
      "Epoch: 9, loss: 5.042504190178776e-16\r",
      "Epoch: 9, loss: 5.777476333472521e-16\r",
      "Epoch: 9, loss: 1.807674004853055e-16\r",
      "Epoch: 9, loss: 4.040873719214028e-16\r",
      "Epoch: 9, loss: 3.7303691251839935e-16\r",
      "Epoch: 9, loss: 5.274702614196612e-16\r",
      "Epoch: 9, loss: 4.6814761349100216e-18\r",
      "Epoch: 9, loss: 7.848891824564803e-17\r",
      "Epoch: 9, loss: 2.8232582032357857e-16\r",
      "Epoch: 9, loss: 1.8457850967111258e-16\r",
      "Epoch: 9, loss: 1.172394566753445e-17\r",
      "Epoch: 9, loss: 9.646244389298751e-17\r",
      "Epoch: 9, loss: 1.9164196408239735e-16\r",
      "Epoch: 9, loss: 8.06667872798709e-18\r",
      "Epoch: 9, loss: 1.3272608849542936e-16\r",
      "Epoch: 9, loss: 1.82549181208653e-16\r",
      "Epoch: 9, loss: 8.213116809507985e-17\r",
      "Epoch: 9, loss: 7.297439422263879e-17\r",
      "Epoch: 9, loss: 3.5094594844978917e-16\r",
      "Epoch: 9, loss: 4.034826641945383e-17\r",
      "Epoch: 10, loss: 2.7265684657325277e-16\r",
      "Epoch: 10, loss: 8.309239873328736e-17\r",
      "Epoch: 10, loss: 2.6054650173997413e-16\r",
      "Epoch: 10, loss: 3.016427131755307e-16\r",
      "Epoch: 10, loss: 1.7038035841578385e-16\r",
      "Epoch: 10, loss: 6.537143864710735e-17\r",
      "Epoch: 10, loss: 8.274250439104748e-17\r",
      "Epoch: 10, loss: 1.489647896343514e-16\r",
      "Epoch: 10, loss: 7.174908414307867e-17\r",
      "Epoch: 10, loss: 1.4511469054402503e-16\r",
      "Epoch: 10, loss: 2.2269471836080622e-17\r",
      "Epoch: 10, loss: 1.4482083273566702e-17\r",
      "Epoch: 10, loss: 1.986280170339882e-18\r",
      "Epoch: 10, loss: 1.2465820879498999e-16\r",
      "Epoch: 10, loss: 1.4801484505914552e-16\r",
      "Epoch: 10, loss: 5.120354964610524e-17\r",
      "Epoch: 10, loss: 2.0622048092111133e-16\r",
      "Epoch: 10, loss: 1.2992008980122012e-16\r",
      "Epoch: 10, loss: 1.5807726748121663e-16\r",
      "Epoch: 10, loss: 9.601375308562218e-17\r",
      "Epoch: 10, loss: 5.559332019212132e-17\r",
      "Epoch: 10, loss: 1.1349471978146722e-16\r",
      "Epoch: 10, loss: 5.289051434849749e-17\r",
      "Epoch: 10, loss: 1.4562425151085945e-16\r",
      "Epoch: 10, loss: 7.150060899894623e-17\r",
      "Epoch: 10, loss: 8.947817196748818e-17\r",
      "Epoch: 10, loss: 1.4864190845597107e-17\r",
      "Epoch: 10, loss: 4.537904131492869e-17\r",
      "Epoch: 10, loss: 4.2353408309253745e-17\r",
      "Epoch: 10, loss: 2.6903385903376278e-17\r",
      "Epoch: 10, loss: 1.8429494673743427e-16\r",
      "Epoch: 10, loss: 4.578202942498258e-17\r",
      "Epoch: 10, loss: 9.656215177898465e-17\r",
      "Epoch: 10, loss: 2.6177349113309937e-17\r",
      "Epoch: 10, loss: 9.809961348885958e-17\r",
      "Epoch: 10, loss: 1.2886933969550401e-17\r",
      "Epoch: 10, loss: 5.2154031078306047e-17\r",
      "Epoch: 10, loss: 2.1121936607628817e-17\r",
      "Epoch: 10, loss: 5.402485631756801e-17\r",
      "Epoch: 10, loss: 5.1289246965044704e-18\r",
      "Epoch: 10, loss: 3.7061907833881785e-17\r",
      "Epoch: 10, loss: 2.3285095075505908e-17\r",
      "Epoch: 10, loss: 1.5329438385569898e-16\r",
      "Epoch: 10, loss: 1.8053363262903985e-16\r",
      "Epoch: 10, loss: 2.1983935030613124e-17\r",
      "Epoch: 10, loss: 5.82766173329576e-17\r",
      "Epoch: 10, loss: 2.3714177450033807e-17\r",
      "Epoch: 10, loss: 1.0240294502090142e-16\r",
      "Epoch: 10, loss: 1.1196755788826276e-17\r",
      "Epoch: 10, loss: 6.25742484191716e-17\r",
      "Epoch: 10, loss: 4.167477734768269e-17\r",
      "Epoch: 10, loss: 9.23957413917148e-17\r",
      "Epoch: 10, loss: 3.192537605986934e-17\r",
      "Epoch: 10, loss: 1.3707461261956536e-17\r",
      "Epoch: 10, loss: 1.3134937632300482e-17\r",
      "Epoch: 10, loss: 3.083101307420092e-17\r",
      "Epoch: 10, loss: 4.193192802205939e-17\r",
      "Epoch: 10, loss: 2.204007425701685e-17\r",
      "Epoch: 10, loss: 6.768871677035472e-17\r",
      "Epoch: 10, loss: 1.905735158653034e-17\r",
      "Epoch: 10, loss: 8.884821223559925e-19\r",
      "Epoch: 10, loss: 7.615782802838974e-17\r",
      "Epoch: 10, loss: 2.735184312612574e-17\r",
      "Epoch: 10, loss: 8.948297527212263e-18\r",
      "Epoch: 10, loss: 7.291581447467896e-17\r",
      "Epoch: 10, loss: 8.897593332394518e-17\r",
      "Epoch: 10, loss: 4.6992830369622856e-17\r",
      "Epoch: 10, loss: 9.083956077724413e-18\r",
      "Epoch: 10, loss: 6.084928245423061e-19\r",
      "Epoch: 10, loss: 7.08516600208879e-18\r",
      "Epoch: 10, loss: 4.394812833632528e-19\r",
      "Epoch: 10, loss: 4.000426106408667e-18\r",
      "Epoch: 10, loss: 8.912831608661196e-19\r",
      "Epoch: 10, loss: 6.609769012457943e-17\r",
      "Epoch: 10, loss: 2.0398594198157356e-18\r",
      "Epoch: 10, loss: 7.710875955392417e-17\r",
      "Epoch: 10, loss: 1.1267033184108535e-17\r",
      "Epoch: 10, loss: 5.217432046524306e-17\r",
      "Epoch: 10, loss: 3.133174233388271e-17\r",
      "Epoch: 10, loss: 2.3174670033621905e-17\r",
      "Epoch: 10, loss: 6.827569349100931e-17\r",
      "Epoch: 10, loss: 7.778555339253078e-17\r",
      "Epoch: 10, loss: 2.4877738839751704e-17\r",
      "Epoch: 10, loss: 5.487716844645257e-17\r",
      "Epoch: 10, loss: 5.045484018814064e-17\r",
      "Epoch: 10, loss: 7.087551725885678e-17\r",
      "Epoch: 10, loss: 7.707862438965508e-19\r",
      "Epoch: 10, loss: 1.0980262784696145e-17\r",
      "Epoch: 10, loss: 3.856166820419703e-17\r",
      "Epoch: 10, loss: 2.5185732459105476e-17\r",
      "Epoch: 10, loss: 1.8465225951240827e-18\r",
      "Epoch: 10, loss: 1.3452245857475526e-17\r",
      "Epoch: 10, loss: 2.6224851169354583e-17\r",
      "Epoch: 10, loss: 1.273604400053099e-18\r",
      "Epoch: 10, loss: 1.8325118029959645e-17\r",
      "Epoch: 10, loss: 2.5085380774231404e-17\r",
      "Epoch: 10, loss: 1.1472465579187026e-17\r",
      "Epoch: 10, loss: 1.0228722966499656e-17\r",
      "Epoch: 10, loss: 4.752062299815825e-17\r",
      "Epoch: 10, loss: 5.759133144955995e-18\r"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(model2.parameters(), lr=0.01, lr_schedulers=None, momentum=0.9, dampening=0, \n",
    "                weight_decay=0, nesterov=False, maximize=False)\n",
    "criterion = MSELoss()\n",
    "\n",
    "EPOCHS = 10\n",
    "total_loss_2 = []\n",
    "for epoch in range(EPOCHS):\n",
    "    for input, target in zip(inputs, targets):\n",
    "        output = model2(input)\n",
    "        loss = criterion(target, output)\n",
    "        model2.backward(criterion.backward())\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"Epoch: {epoch+1}, loss: {loss.item()}\", end='\\r')\n",
    "        total_loss_2.append(loss.item())\n",
    "    optimizer.schedule_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "id": "5a6bb414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter([[1.99987928e+00, 3.75557679e-05],\n",
       "              [1.92420724e-04, 1.99984504e+00]]) with gradient: Tensor([[0., 0.],\n",
       "           [0., 0.]]),\n",
       "   Parameter([[5.26429666e-07],\n",
       "              [6.59687275e-05]]) with gradient: Tensor([[0.],\n",
       "           [0.]])],\n",
       "  'lr': 0.01,\n",
       "  'lr_schedulers': None,\n",
       "  'momentum': 0.9,\n",
       "  'dampening': 0,\n",
       "  'weight_decay': 0,\n",
       "  'nesterov': False,\n",
       "  'maximize': False,\n",
       "  'grads_ema': [Tensor([[-3.21468088,  0.32562826],\n",
       "           [-3.01793891,  0.30569945]]),\n",
       "   Tensor([[-1.75592288],\n",
       "           [-1.64845849]])]}]"
      ]
     },
     "execution_count": 795,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "id": "44567804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x183fcda7d30>"
      ]
     },
     "execution_count": 796,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(total_loss_1, label='without momentum')\n",
    "plt.plot(total_loss_2, label='with momentum')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "id": "9e3029a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weights',\n",
       "  Parameter([[-0.77913514,  1.86313789],\n",
       "             [ 0.67635851, -0.2000684 ],\n",
       "             [ 0.14666837, -0.06879656]]) with gradient: Tensor([[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]])),\n",
       " ('bias',\n",
       "  Parameter([[ 1.00000000e+00],\n",
       "             [ 8.89586397e-31],\n",
       "             [-1.00000000e+00]]) with gradient: Tensor([[0.],\n",
       "          [0.],\n",
       "          [0.]]))]"
      ]
     },
     "execution_count": 716,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defe3475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce \n",
    "\n",
    "\n",
    "class LrOptimizer(Optimizer):\n",
    "    \n",
    "    def __init__(self, params: Union[Iterator[Tuple[str, Parameter]], Dict[str, Any]], \n",
    "                 **defaults):\n",
    "        self.param_groups = Optimizer._group_params(params=params, defaults=defaults)\n",
    "         \n",
    "    def _group_params(params, defaults: Union[dict, None] = None) -> list[dict]:\n",
    "         # Convert iterable of parameters into list of dictionaries\n",
    "        param_groups = list(params) \n",
    "        if all(isinstance(item, Parameter) for item in param_groups):\n",
    "            param_groups = [{'params': param_groups}]\n",
    "            \n",
    "        # Convert single Parameters into a list of one Parameter and other iterables into lists \n",
    "        for param_group in param_groups:   \n",
    "            params = param_group['params']\n",
    "            if isinstance(params, Parameter):\n",
    "                params = [params]\n",
    "            else:\n",
    "                params = list(params)\n",
    "                if len(params) == 0:\n",
    "                    raise ValueError(\"Empty parameter list passed to Optimizer\")\n",
    "            param_group['params'] = params\n",
    "            \n",
    "        # Add defaults if present \n",
    "        if defaults is not None: \n",
    "            for param_group in param_groups:\n",
    "                keys_present = param_group.keys() \n",
    "                param_group.update({key: val for key, val in defaults.items()\n",
    "                                    if key not in keys_present})\n",
    "        return param_groups\n",
    "        \n",
    "    @abstractmethod \n",
    "    def step(self) -> None:\n",
    "        ... \n",
    "        \n",
    "    def schedule_lr(self) -> None:\n",
    "        \"\"\"\n",
    "        Still need to convert lr_schedulers into a list \n",
    "        \"\"\"\n",
    "        for param_group in self.param_groups:\n",
    "            if 'lr_schedulers' in param_group.keys():\n",
    "                param_group['lr'] = reduce(\n",
    "                    lambda lr, lr_scheduler: lr_scheduler.step(lr), \n",
    "                    param_group['lr_schedulers'], \n",
    "                    param_group['lr'],\n",
    "                )\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        for param_group in self.param_groups:\n",
    "            for param in param_group['params']:\n",
    "                param.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "6dad529e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter([[ 1.67057228,  1.87517728],\n",
       "              [ 0.13839523, -1.03497162],\n",
       "              [-0.80922372, -0.29975701]]) with gradient: Tensor([[0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.]]),\n",
       "   Parameter([[-0.25474946],\n",
       "              [-1.51037068],\n",
       "              [-0.17723706]]) with gradient: Tensor([[0.],\n",
       "           [0.],\n",
       "           [0.]])],\n",
       "  'lr': 0.01,\n",
       "  'lr_schedulers': None,\n",
       "  'momentum': 0,\n",
       "  'dampening': 0.1,\n",
       "  'weight_decay': 0,\n",
       "  'nesterov': True,\n",
       "  'maximize': False,\n",
       "  'grads_ema': None}]"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
